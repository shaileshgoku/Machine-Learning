{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"InterviewQuestions.ipynb","provenance":[],"collapsed_sections":["NdjmOW_IByk1","zi8NhtGHEHz1","a0pxJnkjByiS","I2fTQgakByfr","awpt6aoEBydx","nk2ZFOIPByc5","E_W0T_doByX5","MtALhD7yByVI","2htEkrudBySj","PLBM1I3rByP-","wby90wQ9ByNS","71ez8jjRByKt","eLTUNDptByIQ","SOkOd9cOByFO","_Y44z3-rByCp","qR7yKydQ1lF0","SN4g1E6M1k8x","77DqEaJ61kqd","ZfGye1cS1kka","bjKJJkBC1khD","A3sQL12gxbAa","7it-UPTFxa94","HtZMJI1gxa7-","5ejza4r1xa6B","A-joMVDExayX","Fpop1UK1xavl","NeuE8y0uxasd","jlcDjaj61kbU","L2xAuHjg1kUx","PayKzWpdClUv","hsuEuVK0ClRz","euVg9lUCClOp","V3Wy9NDFClL0","TPG8L9lu3p91","RYz74MgE3p7A","Vv-YVFKWC3Bw","0yexazgbC3AP","EtCJpa7-C2rw","lvufrGi73p1U","JSzefg6V5FwW"],"authorship_tag":"ABX9TyMBHtIJBq3IWoYEiieQRanh"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"hvxWoCYZ89xt"},"source":["# Basic python\n","\n","##Difference between set, tuples, dictionary and list\n","\n","|Set|Tuple|Dictionary|List|\n","|-|-|-|-|\n","|Set is mutable|Tuple is immutable|Dictionary is mutable. But Keys are not duplicated.|It is mutable|\n","|It is ordered collection of items|It is ordered collection of items|It is ordered|It is ordered collection of items|\n","|Set can be created using set() function|Tuple can be created using tuple() function.|Dictionary can be created using dict() function.|List can be created using list() function|\n","|Items in set cannot be changed or replaced|Items in tuple cannot be changed or replaced|Items in the list can be replaced or changed|Items in the list can be replaced or changed|\n","|Set will not allow duplicate elements|Tuple allows duplicate elements|Keys are not duplicated but values can be duplicated|List allows duplicate elements|\n","|Set can be represented by { }|Tuple can be represented by  ( )|Dictionary  can be represented by { }|List can be represented by [ ]|\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"YQeqjRl189sz"},"source":["##Slicing - string reverse\n","\n","Python slicing is about obtaining a sub-string from the given string by slicing it respectively from start to end.\n","\n","```python\n","string[start:end:step]\n","```\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C-thYD5X_oeS","executionInfo":{"status":"ok","timestamp":1628253891288,"user_tz":-330,"elapsed":413,"user":{"displayName":"Jay Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghp15R1mmnoYYCixHxYvGt-QFWUeep9kyrxYR5p9A=s64","userId":"15713361043153944297"}},"outputId":"a8173e1f-526e-471c-b585-f29dd7003566"},"source":["string= 'Hello World'\n","print(string[::-1])"],"execution_count":9,"outputs":[{"output_type":"stream","text":["dlroW olleH\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Dvab3Qvs89rL"},"source":["##Map and filter function\n","\n","The basic function of map() is to manipulate iterables. Map executes all the conditions of a function on the items in the iterable.\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b5dpGl6L_zNf","executionInfo":{"status":"ok","timestamp":1628178458220,"user_tz":-330,"elapsed":411,"user":{"displayName":"Jay Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghp15R1mmnoYYCixHxYvGt-QFWUeep9kyrxYR5p9A=s64","userId":"15713361043153944297"}},"outputId":"6cebfef4-1f30-4572-a2c4-8ab691918b94"},"source":["array= [2,3,5,6,8]\n","list(map(lambda arr: arr*2, array))\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[4, 6, 10, 12, 16]"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"wUA53Cdw_91x"},"source":["Filter is used to filter the iterables as per the conditions. Filter filters the original iterable and passes the items that returns True for the function provided to filter. Therefore only the items in the iterables can be expected to be seen in the output. \n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mH8jkr4-_zJA","executionInfo":{"status":"ok","timestamp":1628178515257,"user_tz":-330,"elapsed":385,"user":{"displayName":"Jay Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghp15R1mmnoYYCixHxYvGt-QFWUeep9kyrxYR5p9A=s64","userId":"15713361043153944297"}},"outputId":"dd2eecda-b6cd-47bf-beb7-4d6d51ecf772"},"source":["array= [2,3,5,6,8]\n","list(filter(lambda arr: arr%2==0, array))\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[2, 6, 8]"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"OKzqUuEF89qt"},"source":["##Lambda function\n","\n","Lambda function is a small anonymous function. It can take any number of arguments, but can only have one expression.\n","\n","```python\n","lambda arguments : expression\n","```\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FHGCMbK1AWBu","executionInfo":{"status":"ok","timestamp":1628178578019,"user_tz":-330,"elapsed":375,"user":{"displayName":"Jay Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghp15R1mmnoYYCixHxYvGt-QFWUeep9kyrxYR5p9A=s64","userId":"15713361043153944297"}},"outputId":"1b1a267c-5af9-4841-f7a6-e9dc9d547c0b"},"source":["x = lambda a, b : a * b\n","print(x(5, 6))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["30\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"-AiGV3nD89k8"},"source":["##OOps :- Class, module , over riding and over loading\n","\n","Python is also an object-oriented language since its beginning. It allows us to develop applications using an Object-Oriented approach. An object-oriented paradigm is to design the program using classes and objects. The object is related to real-word entities such as book, house, pencil, etc. The oops concept focuses on writing the reusable code.\n","\n","**Class:**\n","\n","The class can be defined as a collection of objects. It is a logical entity that has some specific attributes and methods. For example: if you have an employee class, then it should contain an attribute and method, i.e. an email id, name, age, salary, etc.\n","\n","```python \n","class ClassName:     \n","        <statement-1>     \n","        .     \n","        .      \n","        <statement-N> \n","\n","```\n","\n","\n","**Module:**\n","\n","A module is a file containing Python definitions and statements. Module is Python code that can be called from other programs for commonly used tasks, without having to type them in each and every program that uses them.\n","\n","\n","**Overriding:**\n","\n","Method overriding is an example of run time polymorphism. In this, the specific implementation of the method that is already provided by the parent class is provided by the child class. It is used to change the behavior of existing methods and there is a need for at least two classes for method overriding. In method overriding, inheritance always required as it is done between parent class(superclass) and child class(child class) methods.\n","\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FblFpb8qA4mF","executionInfo":{"status":"ok","timestamp":1628178719938,"user_tz":-330,"elapsed":478,"user":{"displayName":"Jay Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghp15R1mmnoYYCixHxYvGt-QFWUeep9kyrxYR5p9A=s64","userId":"15713361043153944297"}},"outputId":"768be899-91a2-4b3c-ba1d-0253bd78bdf4"},"source":["class A:\n"," \n","    def fun1(self):\n","        print('feature_1 of class A')\n","    def fun2(self):\n","        print('feature_2 of class A')\n","     \n"," \n","class B(A):\n","     \n","    # Modified function that is\n","    # already exist in class A\n","    def fun1(self):\n","        print('Modified feature_1 of class A by class B')   \n","         \n","    def fun3(self):\n","        print('feature_3 of class B')\n","         \n"," \n","# Create instance\n","obj = B()\n","     \n","# Call the override function\n","obj.fun1()\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Modified feature_1 of class A by class B\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"3jfqIa-l89kS"},"source":["**Overloading:**\n","\n","Method Overloading is an example of Compile time polymorphism. In this, more than one method of the same class shares the same method name having different signatures. Method overloading is used to add more to the behavior of methods and there is no need of more than one class for method overloading.\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o0D8Uy0XBIcL","executionInfo":{"status":"ok","timestamp":1628253879204,"user_tz":-330,"elapsed":412,"user":{"displayName":"Jay Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghp15R1mmnoYYCixHxYvGt-QFWUeep9kyrxYR5p9A=s64","userId":"15713361043153944297"}},"outputId":"592774c7-d684-4037-daba-a87ea648f4f5"},"source":["# Function to take multiple arguments\n","def add(datatype, *args):\n","\n","\t# if datatype is int\n","\t# initialize answer as 0\n","\tif datatype =='int':\n","\t\tanswer = 0\n","\t\t\n","\t# if datatype is str\n","\t# initialize answer as ''\n","\tif datatype =='str':\n","\t\tanswer =''\n","\n","\t# Traverse through the arguments\n","\tfor x in args:\n","\n","\t\tanswer = answer + x\n","\n","\tprint(answer)\n","\n","# Integer\n","add('int', 5, 6)\n","\n","# String\n","add('str', 'Hello ', 'World')\n"],"execution_count":8,"outputs":[{"output_type":"stream","text":["11\n","Hello World\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"xlzX-s1i8z4n"},"source":["##Using Numpy - some aggregation ( greater than or less than )\n","\n","The Python numpy comparison operators and functions used to compare the array items and returns Boolean True or false. The Python Numpy comparison functions are greater, greater_equal, less, less_equal, equal, and not_equal. Like any other, Python Numpy comparison operators are <, <=, >, >=, == and !=\n","\n","\n","Python numpy Array greater\n","It is a simple Python Numpy Comparison Operators example to demonstrate the Python Numpy greater function. First, we declared an array of random elements. Next, we are checking whether the elements in an array are greater than 0, greater than 1 and 2. If True, True returned otherwise, False returned. \n","```python\n","numpy.less(array_name, integer_value)\n","```"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XzyeASoB7MMh","executionInfo":{"status":"ok","timestamp":1628177307433,"user_tz":-330,"elapsed":501,"user":{"displayName":"Jay Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghp15R1mmnoYYCixHxYvGt-QFWUeep9kyrxYR5p9A=s64","userId":"15713361043153944297"}},"outputId":"f47531b9-45a0-4fa4-9157-d213a5cb6485"},"source":["import numpy as np\n","\n","arr1 = np.random.randint(10, 50, size = (5, 8))\n","\n","arr2 = np.random.randint(1, 20, size = (2, 3, 6))\n","\n","x = np.array([0, 2, 3, 0, 1, 6, 5, 2])\n","print('Original Array = ', x)\n","print('\\nGreater Than 0 = ', np.greater(x, 2))\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Original Array =  [0 2 3 0 1 6 5 2]\n","\n","Greater Than 0 =  [False False  True False False  True  True False]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iGJuucTA7Urh","executionInfo":{"status":"ok","timestamp":1628177341040,"user_tz":-330,"elapsed":413,"user":{"displayName":"Jay Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghp15R1mmnoYYCixHxYvGt-QFWUeep9kyrxYR5p9A=s64","userId":"15713361043153944297"}},"outputId":"54c21694-90c8-45e0-a1cc-77c75ff912bb"},"source":["arr2 = np.random.randint(1, 15, size = (2, 3, 6))\n","print('\\n-----Three Dimensional Random Array----')\n","print(arr2)\n","print()\n","print(np.greater_equal(arr2, 7))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","-----Three Dimensional Random Array----\n","[[[11  2  9  8  6  6]\n","  [ 2  7 13 13 12  3]\n","  [ 2  3  4  2 10 11]]\n","\n"," [[ 2  5 11  3  3  9]\n","  [ 8 14 12  9  6  6]\n","  [14 10  8  8 12  1]]]\n","\n","[[[ True False  True  True False False]\n","  [False  True  True  True  True False]\n","  [False False False False  True  True]]\n","\n"," [[False False  True False False  True]\n","  [ True  True  True  True False False]\n","  [ True  True  True  True  True False]]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"aIZI8pBY8Upt"},"source":["The Python Numpy less function checks whether the elements in a given array is less than a specified number or not. If True, boolean True returned otherwise, False. The syntax of this Python Numpy less function is\n","\n","```python\n","numpy.less(array_name, integer_value)\n","```\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mxd7c5aE7pZc","executionInfo":{"status":"ok","timestamp":1628177369740,"user_tz":-330,"elapsed":567,"user":{"displayName":"Jay Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghp15R1mmnoYYCixHxYvGt-QFWUeep9kyrxYR5p9A=s64","userId":"15713361043153944297"}},"outputId":"fa27cc11-587b-4541-b220-21cee969019f"},"source":["arr2 = np.random.randint(1, 25, size = (2, 3, 6))\n","print('\\n-----Three Dimensional Random Array----')\n","print(arr2)\n","print()\n","print(np.less(arr2, 15))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","-----Three Dimensional Random Array----\n","[[[19 11  5 10 16 24]\n","  [17  2 23  4  2  1]\n","  [10 19 14 21 18 24]]\n","\n"," [[ 1 14  6  1  6 20]\n","  [ 8  8  4  2 16 10]\n","  [ 5  7 17 13  8  8]]]\n","\n","[[[False  True  True  True False False]\n","  [False  True False  True  True  True]\n","  [ True False  True False False False]]\n","\n"," [[ True  True  True  True  True False]\n","  [ True  True  True  True False  True]\n","  [ True  True False  True  True  True]]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"-huYVhKuByqW"},"source":["##We need a new column appended to the data frame : avg (revenue) per state without disrupting the original data frame (no data loss/ duplication side effect allowed)\n","\n","|State|City|Revenue|\n","|-|-|-|\n","|A|a|12|\n","|A|g|10|\n","|B|b|13|\n","|C|c|14|\n","|C|d|10|\n","|D|q|15|\n","\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":289},"id":"sDKa1cKmf6Dx","executionInfo":{"status":"ok","timestamp":1628256240653,"user_tz":-330,"elapsed":388,"user":{"displayName":"Jay Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghp15R1mmnoYYCixHxYvGt-QFWUeep9kyrxYR5p9A=s64","userId":"15713361043153944297"}},"outputId":"e938d39e-49ea-4885-d736-c26baed1661d"},"source":["import pandas as pd\n","\n","data= [['A', 'a', 12],\n","       ['A', 'g', 10],\n","       ['B', 'b', 13],\n","       ['C', 'c', 14],\n","       ['C', 'd', 10],\n","       ['D', 'q', 15]]\n","\n","df= pd.DataFrame(data, columns= ['State', 'City', 'Revenue'])\n","df['avg (revenue)']= 'NaN'\n","\n","states= df['State'].unique()\n","total_rev= {}\n","for state in states:\n","  revenue= df[df['State']==state]['Revenue']\n","  df['avg (revenue)'].loc[df.State==state]= revenue.sum()/len(revenue)\n","  \n","\n","\n","x= df[df['State']=='A']['Revenue']\n","df.head()\n"],"execution_count":59,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py:670: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  iloc._setitem_with_indexer(indexer, value)\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>State</th>\n","      <th>City</th>\n","      <th>Revenue</th>\n","      <th>avg (revenue)</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>A</td>\n","      <td>a</td>\n","      <td>12</td>\n","      <td>11</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>A</td>\n","      <td>g</td>\n","      <td>10</td>\n","      <td>11</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>B</td>\n","      <td>b</td>\n","      <td>13</td>\n","      <td>13</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>C</td>\n","      <td>c</td>\n","      <td>14</td>\n","      <td>12</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>C</td>\n","      <td>d</td>\n","      <td>10</td>\n","      <td>12</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  State City  Revenue avg (revenue)\n","0     A    a       12            11\n","1     A    g       10            11\n","2     B    b       13            13\n","3     C    c       14            12\n","4     C    d       10            12"]},"metadata":{"tags":[]},"execution_count":59}]},{"cell_type":"code","metadata":{"id":"e4B56BHjhPXW","executionInfo":{"status":"ok","timestamp":1628256274693,"user_tz":-330,"elapsed":411,"user":{"displayName":"Jay Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghp15R1mmnoYYCixHxYvGt-QFWUeep9kyrxYR5p9A=s64","userId":"15713361043153944297"}}},"source":[""],"execution_count":59,"outputs":[]},{"cell_type":"code","metadata":{"id":"wxLrlpPaCO-W"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Mcebk3JkCO7o"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OpR5UPuDBynh"},"source":["##Final Data Frame would contain one row per department (with only the third highest salaried employee of the department)\n","\n","\n","|Employee ID|Department|Salary|\n","|-|-|-|\n","|1|A|2000|\n","|2|B|1000|\n","|3|A|5000|\n","|4|C|4000|\n","|5|B|10000|\n","|6|D|2000|\n","\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":230},"id":"G0Va5m_to2ZN","executionInfo":{"status":"ok","timestamp":1628258802910,"user_tz":-330,"elapsed":398,"user":{"displayName":"Jay Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghp15R1mmnoYYCixHxYvGt-QFWUeep9kyrxYR5p9A=s64","userId":"15713361043153944297"}},"outputId":"bff68941-462d-4ed8-fa45-cdcf06b900b8"},"source":["import pandas as pd\n","\n","data= [['1', 'A', 2000],\n","       ['2', 'B', 1000],\n","       ['3', 'A', 5000],\n","       ['4', 'C', 4000],\n","       ['5', 'B', 10000],\n","       ['6', 'D', 2000]]\n","\n","df= pd.DataFrame(data, columns=['Employee ID', 'Department', 'Salary'])\n","print(df)\n","\n","final_df= pd.DataFrame(columns=['Employee ID', 'Department', 'Salary'])\n","\n","unique_dept= df['Department'].unique()\n","\n","salary_index= 1 # If want third highest salary change it to 2\n","\n","for dept in unique_dept:\n","  salary= df[df['Department']==dept]['Salary']\n","  if len(salary)>salary_index:\n","    \n","    ind= salary.sort_values(ascending=False).index[salary_index]\n","    val= salary.sort_values(ascending=False).values[salary_index]\n","\n","    final_df.loc[len(final_df.index)]= [ind, dept, val]\n","\n","final_df.head()"],"execution_count":125,"outputs":[{"output_type":"stream","text":["  Employee ID Department  Salary\n","0           1          A    2000\n","1           2          B    1000\n","2           3          A    5000\n","3           4          C    4000\n","4           5          B   10000\n","5           6          D    2000\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Employee ID</th>\n","      <th>Department</th>\n","      <th>Salary</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>A</td>\n","      <td>2000</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>B</td>\n","      <td>1000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  Employee ID Department Salary\n","0           0          A   2000\n","1           1          B   1000"]},"metadata":{"tags":[]},"execution_count":125}]},{"cell_type":"code","metadata":{"id":"4TFxmNtUrMsr","executionInfo":{"status":"ok","timestamp":1628258835208,"user_tz":-330,"elapsed":598,"user":{"displayName":"Jay Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghp15R1mmnoYYCixHxYvGt-QFWUeep9kyrxYR5p9A=s64","userId":"15713361043153944297"}}},"source":[""],"execution_count":125,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ilwy2-X3Cl-Y"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WIGDryqGCl9x"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NdjmOW_IByk1"},"source":["##Relu function using python\n","\n","\n","\n","Return 0 if the input is negative otherwise return the input as it is.\n","\n","We can represent it mathematically:\n","\n","\n","$$\n","f(x) = max(0,x) = \\left\\{\n","    \\begin{array}\\\\\n","        x_i & \\mbox{if } \\ x_i > 0 \\\\\n","        0 & \\mbox{if } \\ x < 0\n","    \\end{array}\n","\\right.\n","$$\n","\n","\n","The pseudo code for Relu is:\n","\n","```python\n","if input > 0:\n","    return input\n","else:\n","    return 0\n","```\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZFBfu73eIn2T","executionInfo":{"status":"ok","timestamp":1628180784859,"user_tz":-330,"elapsed":413,"user":{"displayName":"Jay Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghp15R1mmnoYYCixHxYvGt-QFWUeep9kyrxYR5p9A=s64","userId":"15713361043153944297"}},"outputId":"ac7fa36d-91de-4dd7-d297-88da67c0f7ec"},"source":["def relu(x):\n","    return max(0.0, x)\n","\n","x = 2.0\n","print('Applying Relu on (%.1f) gives %.1f' % (x, relu(x)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Applying Relu on (2.0) gives 2.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Dn641ukyEIW6"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5wSM160SEdqr"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zi8NhtGHEHz1"},"source":["##Stratified and cluster sampling\n","\n","\n","|Stratified Sampling|Cluster Sampling|\n","|-|-|\n","|Eements within each stratum are sampled.|Sampling is done on a population of clusters therefore, cluster/group is considered a sampling unit.|\n","|Fom each stratum, a random sample is selected.|Only selected clusters are sampled.|\n","|The motive is to increase precision to reduce error.|The aim is to reduce cost and increase the efficiency of sampling.|"]},{"cell_type":"markdown","metadata":{"id":"a0pxJnkjByiS"},"source":["##Different distribution techniques\n","\n","**Bernoulli Distribution**\n","\n","A Bernoulli distribution has only two possible outcomes, namely 1 (success) and 0 (failure), and a single trial. So the random variable X which has a Bernoulli distribution can take value 1 with the probability of success, say p, and the value 0 with the probability of failure, say q or 1-p.\n","\n","The probability mass function is given by: \n","\n","$p^x(1-p)^{1-x} \\ {where} \\ x \\in \\mathbf (0, 1)$\n","\n","It can also be written as\n","\n","\n","$$\n","P(x) = \\left\\{\n","    \\begin{array}\\\\\n","        1-p & \\mbox{, } \\ x = 0 \\\\\n","        p & \\mbox{, } \\ x = 1 \n","    \\end{array}\n","\\right.\n","$$\n","\n","\n","The expected value of a random variable X from a Bernoulli distribution is found as follows:\n","```bash\n","E(X) = 1*p + 0*(1-p) = p\n","```\n","The variance of a random variable from a bernoulli distribution is:\n","```bash\n","V(X) = E(X²) – [E(X)]² = p – p² = p(1-p)\n","```\n","\n","\n","**Uniform Distribution**\n","\n","When you roll a fair die, the outcomes are 1 to 6. The probabilities of getting these outcomes are equally likely and that is the basis of a uniform distribution. Unlike Bernoulli Distribution, all the n number of possible outcomes of a uniform distribution are equally likely.\n","\n","A variable X is said to be uniformly distributed if the density function is:\n","\n","\n","$$f(x) = \\frac{1}{b-a}$$\n","\n","$$for -\\infty<a \\leq {x} \\leq {b} < \\infty {}$$\n","\n","\n","The mean and variance of X following a uniform distribution is:\n","```bash\n","Mean -> E(X) = (a+b)/2\n","\n","Variance -> V(X) =  (b-a)²/12\n","```\n","The standard uniform density has parameters a = 0 and b = 1, so the PDF for standard uniform density is given by:\n","\n","$$\n","f(x) = \\left\\{\n","    \\begin{array}\\\\\n","        1-p & \\mbox{, } \\ 0 \\leq x \\leq 1 \\\\\n","        0 & \\mbox{, } \\ otherwise \n","    \\end{array}\n","\\right.\n","$$\n","\n","\n","**Binomial Distribution**\n","\n","A distribution where only two outcomes are possible, such as success or failure, gain or loss, win or lose and where the probability of success and failure is same for all the trials is called a Binomial Distribution.\n","\n","The properties of a Binomial Distribution are\n","\n","* Each trial is independent.\n","* There are only two possible outcomes in a trial- either a success or a failure.\n","* A total number of n identical trials are conducted.\n","* The probability of success and failure is same for all trials.\n","\n","The mathematical representation of binomial distribution is: \n","$$P(x)= \\frac{n!}{(n-x)!x!}p^xq^{n-x}$$\n","\n","The mean and variance of a binomial distribution are given by:\n","```bash\n","Mean -> µ = n*p\n","\n","Variance -> Var(X) = n*p*q\n","```\n","\n","**Normal Distribution**\n","\n","\n","\n","Normal distribution represents the behavior of most of the situations in the universe (That is why it’s called a “normal” distribution. I guess!). The large sum of (small) random variables often turns out to be normally distributed, contributing to its widespread application. Any distribution is known as Normal distribution if it has the following characteristics:\n","\n","* The mean, median and mode of the distribution coincide.\n","* The curve of the distribution is bell-shaped and symmetrical about the line x=μ.\n","* The total area under the curve is 1.\n","* Exactly half of the values are to the left of the center and the other half to the right.\n","\n","The PDF of a random variable X following a normal distribution is given by:\n","\n","$f(x) = \\frac{1}{ \\sqrt{2 \\pi \\sigma}}e^{-\\frac{1}{2}(\\frac{x-u}{\\sigma})^2}$\n","\n","$$for -\\infty < x< \\infty$$\n","\n","The mean and variance of a random variable X which is said to be normally distributed is given by:\n","\n","\n","```bash\n","Mean -> E(X) = µ\n","\n","Variance -> Var(X) = σ^2\n","```\n","\n","Here, µ (mean) and σ (standard deviation) are the parameters.\n","\n","\n","**Poisson Distribution:**\n","\n","\n","A distribution is called Poisson distribution when the following assumptions are valid:\n","\n","1. Any successful event should not influence the outcome of another successful event.\n","2. The probability of success over a short interval must equal the probability of success over a longer interval.\n","3. The probability of success in an interval approaches zero as the interval becomes smaller.\n","\n","Now, if any distribution validates the above assumptions then it is a Poisson distribution. Some notations used in Poisson distribution are:\n","\n","λ is the rate at which an event occurs,\n","t is the length of a time interval,\n","And X is the number of events in that time interval.\n","Here, X is called a Poisson Random Variable and the probability distribution of X is called Poisson distribution.\n","\n","Let µ denote the mean number of events in an interval of length t. Then, µ = λ*t.\n","\n","The PMF of X following a Poisson distribution is given by:\n","\n","\n","$P(X=x)=e^{-\\mu\\frac{\\mu^x}{x!}}$\n","$$for \\ x= 0,1,2,.....$$\n","\n","\n","**Exponential Distribution**\n","\n","Exponential distribution is widely used for survival analysis. From the expected life of a machine to the expected life of a human, exponential distribution successfully delivers the result.\n","\n","A random variable X is said to have an exponential distribution with PDF:\n","\n","$$f(x) =  \\left\\{\n","    \\begin{array}\\\\\n","        \\lambda e-\\lambda x,  x \\geq 0\n","    \\end{array}\n","\\right.\n","$$\n","\n","and parameter λ>0 which is also called the rate.\n","\n","For survival analysis, λ is called the failure rate of a device at any time t, given that it has survived up to t.\n","\n","Mean and Variance of a random variable X following an exponential distribution:\n","```bash\n","Mean -> E(X) = 1/λ\n","\n","Variance -> Var(X) = (1/λ)²\n","```\n"]},{"cell_type":"markdown","metadata":{"id":"I2fTQgakByfr"},"source":["##How do u handle skewness data\n","\n","**Log Transform**\n","\n","Log transformation is most likely the first thing you should do to remove skewness from the predictor.\n","```python\n","np.log() or np.log1p()\n","```\n","\n","**Square Root Transform**\n","\n","The square root sometimes works great and sometimes isn’t the best suitable option.\n","```python\n","np.sqrt()\n","```\n","\n","**Box-Cox Transform**\n","\n","It is just another way of handling skewed data. To use it, your data must be positive\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"awpt6aoEBydx"},"source":["##R square and adjusted r square\n","\n","**R Square** is a basic matrix which tells you about that how much variance is been explained by the model. What happens in a multivariate linear regression is that if you keep on adding new variables, the R square value will always increase irrespective of the variable significance.\n","\n","Higher the R squared, the more variation is explained by your input variables and hence better is your model.\n","\n","However, the problem with R-squared is that it will either stay the same or increase with addition of more variables, even if they do not have any relationship with the output variables. This is where “Adjusted R square” can help\n","\n","**Adjusted R-square** penalizes you for adding variables which do not improve your existing model. What adjusted R square do is calculate R square from only those variables whose addition in the model which are significant.\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"nk2ZFOIPByc5"},"source":["##Linear regression assumptions\n","\n","\n","Regression is a parametric approach. ‘Parametric’ means it makes assumptions about data for the purpose of analysis.\n","\n","\n","Important assumptions in regression analysis:\n","\n","* There should be a linear and additive relationship between dependent variable and independent variable(s). \n","* There should be no correlation between the residual (error) terms. \n","* The independent variables should not be correlated. Absence of this phenomenon is known as multicollinearity.\n","* The error terms must have constant variance.\n","* The error terms must be normally distributed.\n"]},{"cell_type":"markdown","metadata":{"id":"E_W0T_doByX5"},"source":["##Multi collinearity and VIF\n","\n","\n","**Multicollinearity:**\n","\n","This phenomenon exists when the independent variables are found to be moderately or highly correlated.\n","This means that an independent variable can be predicted from another independent variable in a regression model.\n","\n","We can detect multicollinearity using `VIF`:\n","\n","**VIF:** \n","\n","It determines the strength of the correlation between the independent variables. It is predicted by taking a variable and regressing it against every other variable.\n","\n","$R^2$ value is determined to find out how well an independent variable is described by the other independent variables. A high value of $R^2$ means that the variable is highly correlated with the other variables. This is captured by the $VIF$ which is denoted below:\n","\n","\n","$$VIF= \\frac{1}{1-R^2}$$\n","\n","the closer the $R^2$ value to 1, the higher the value of VIF and the higher the multicollinearity with the particular independent variable."]},{"cell_type":"markdown","metadata":{"id":"MtALhD7yByVI"},"source":["##P value\n","Hypothetical frequency called the P-value, also known as the `observed significance level` for the test hypothesis.\n","\n","The $p-value$, or calculated probability, is the probability of finding the observed/extreme results when the `null hypothesis(H0)` of a study given problem is true. If your $p-value$ is less than the chosen significance level then you reject the `null hypothesis` i.e. accept that your sample claims to support the alternative hypothesis."]},{"cell_type":"markdown","metadata":{"id":"2htEkrudBySj"},"source":["##Hypothesis testing\n","\n","Hypothesis are statement about the given problem. `Hypothesis testing` is a statistical method that is used in making a statistical decision using experimental data. Hypothesis testing is basically an assumption that we make about a population parameter. It evaluates two mutually exclusive statements about a population to determine which statement is best supported by the sample data.\n"]},{"cell_type":"markdown","metadata":{"id":"PLBM1I3rByP-"},"source":["##MSE, MAE, RMSE - which one do u choose.\n","\n","\n","**Mean Squared Error (MSE)**\n","\n","Mean Squared Error (MSE) is the average squared difference between the estimated values and true value.\n","\n","```python\n","np.square(np.subtract(Y_true,Y_pred)).mean()\n","```\n","\n","**Mean Absolute Error (MAE)**\n","\n","Mean Absolute Error (MAE) is the average of the absolute value of the errors.\n","\n","```python\n","np.average(np.abs(y_true - y_pred))\n","```\n","\n","**Root Mean Squared Error (RMSE)**\n","\n","RMSE is a very common evaluation metric. It can range between 0 and infinity. Lower values are better.\n","```python\n","np.sqrt(np.mean((y_true - y_pred) ** 2))\n","```\n","\n","---\n","\n","`MSE` is highly biased for higher values. `RMSE` is better in terms of reflecting performance when dealing with large error values. `RMSE` is more useful when lower residual values are preferred.\n","\n","However, if your dataset has outliers then choose MAE over RMSE.\n"]},{"cell_type":"markdown","metadata":{"id":"wby90wQ9ByNS"},"source":["##Cost function for logistic regression\n","\n","The cost function used in Logistic Regression is `Log Loss`.\n","\n","`Log Loss` is an important classification metric based on probabilities. It’s hard to interpret raw log-loss values, but log-loss is still a good metric for comparing models. For any given problem, a lower log loss value means better predictions.\n","\n","`Log Loss` is the negative average of the log of corrected predicted probabilities for each instance.\n","\n","$$Log Loss= - \\frac{1}{N}\\sum_{i=1}^{N} (log(P_i))$$\n","\n","\n","There are three steps to find Log Loss:\n","\n","* To find corrected probabilities.\n","\n","* Take a log of corrected probabilities.\n","\n","* Take the negative average of the values we get in the 2nd step.\n","\n","\n","For logistic regression, the Cost function is defined as:\n","\n","$$\n","Cost(h_\\theta(x),y) = \\left\\{\n","    \\begin{array}\\\\\n","        -log(h_\\theta(x)) & \\mbox{if } \\ y=1 \\\\\n","        -log(1-h_\\theta(x)) & \\mbox{if } \\ y= 0\n","    \\end{array}\n","\\right.\n","$$\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"71ez8jjRByKt"},"source":["##How to select threshold for logistic regression\n","\n","\n","The choice of a threshold depends on the importance of `TPR(True Positive Rate)` and `FPR(False Positive Rate)` classification problem. For example, if your classifier will decide which criminal suspects will receive a death sentence, false positives are very bad (innocents will be killed!). Thus you would choose a threshold that yields a low FPR while keeping a reasonable TPR (so you actually catch some true criminals). If there is no external concern about low TPR or high FPR, one option is to weight them equally by choosing the threshold that maximizes TPR−FPR.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"eLTUNDptByIQ"},"source":["##AUROC curve\n","\n","In Machine Learning, performance measurement is an essential task. For classification problem, we can count on an AUC - ROC Curve. When we need to check or visualize the performance of the multi-class classification problem, we use the `AUC (Area Under The Curve) ROC (Receiver Operating Characteristics)` curve. It is one of the most important evaluation metrics for checking any classification model’s performance. It is also written as AUROC (Area Under the Receiver Operating Characteristics)\n","\n","`AUC - ROC` curve is a performance measurement for the classification problems at various threshold settings. ROC is a probability curve and AUC represents the degree or measure of separability. It tells how much the model is capable of distinguishing between classes. Higher the AUC, the better the model is at predicting 0 classes as 0 and 1 classes as 1.\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"SOkOd9cOByFO"},"source":["##Logit function\n","\n","A Logit function, also known as the log-odds function, is a function that represents probability values from 0 to 1, and negative infinity to infinity. The function is an inverse to the sigmoid function that limits values between 0 and 1 across the Y-axis, rather than the X-axis. Because the Logit function exists within the domain of 0 to 1, the function is most commonly used in understanding probabilities.\n","\n","The logit link function is used to model the probability of 'success' as a function of covariates (e.g., logistic regression).\n","\n"]},{"cell_type":"markdown","metadata":{"id":"_Y44z3-rByCp"},"source":["##Lasso and Ridge regression\n","\n","To make things regular or acceptable is what we mean by the term regularization.\n","\n","Ridge Regression :\n","In Ridge regression, we add a penalty term which is equal to the square of the coefficient. The L2 term is equal to the square of the magnitude of the coefficients. We also add a coefficient  $\\lambda$  to control that penalty term. In this case if  $\\lambda$  is zero then the equation is the basic OLS else if  $\\lambda \\, > \\, 0$ then it will add a constraint to the coefficient. As we increase the value of \\lambda this constraint causes the value of the coefficient to tend towards zero. This leads to both low variance and low bias.\n","\n","\n","$$L_{ridge}= argmin_\\beta(|| Y- \\beta*X||^2 + \\lambda * ||\\beta||_2^2)$$\n","\n","Lasso Regression :\n","Lasso regression stands for Least Absolute Shrinkage and Selection Operator. It adds penalty term to the cost function. This term is the absolute sum of the coefficients. As the value of coefficients increases from 0 this term penalizes, cause model, to decrease the value of coefficients in order to reduce loss. The difference between ridge and lasso regression is that it tends to make coefficients to absolute zero as compared to Ridge which never sets the value of coefficient to absolute zero.\n","\n","$$L_{lasso}= argmin_\\beta(|| Y- \\beta*X||^2 + \\lambda * ||\\beta||_1)$$\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"qR7yKydQ1lF0"},"source":["##How do handle overfitting ?\n","\n","\n","Detecting overfitting is useful, but it doesn’t solve the problem. Fortunately, you have several options to try.\n","\n","Here are a few of the most popular solutions for overfitting:\n","\n","**Cross-validation**\n","\n","Cross-validation is a powerful preventative measure against overfitting.\n","\n","The idea is clever: Use your initial training data to generate multiple mini train-test splits. Use these splits to tune your model.\n","\n","**Train with more data**\n","\n","It won’t work every time, but training with more data can help algorithms detect the signal better.\n","\n","**Remove features**\n","\n","Some algorithms have built-in feature selection.\n","\n","For those that don’t, you can manually improve their generalizability by removing irrelevant input features.\n","\n","**Early stopping**\n","\n","When you’re training a learning algorithm iteratively, you can measure how well each iteration of the model performs.\n","\n","Up until a certain number of iterations, new iterations improve the model. After that point, however, the model’s ability to generalize can weaken as it begins to overfit the training data.\n","\n","**Regularization**\n","\n","Regularization refers to a broad range of techniques for artificially forcing your model to be simpler.\n","\n","**Ensembling**\n","\n","Ensembles are machine learning methods for combining predictions from multiple separate models.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"58gqtm6W1k-s"},"source":["##How do u treat and handle outliers ? - PyOD, Isolation forest, LOF, One class SVM\n","\n","Few methods of dealing with outliers:\n","\n","* Univariate method: This method looks for data points with extreme values on one variable.\n","* Multivariate method: Here we look for unusual combinations on all the variables.\n","* Minkowski error: This method reduces the contribution of potential outliers in the training process.\n","\n","Proximity-based models:\n","\n","These models consider outliers as points which are isolated from rest of observations. The main proximity-based models include:\n","* cluster analysis\n","* density based analysis\n","* nearest neighborhood.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rH12SPxs2Uy6","executionInfo":{"status":"ok","timestamp":1628260010964,"user_tz":-330,"elapsed":482,"user":{"displayName":"Jay Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghp15R1mmnoYYCixHxYvGt-QFWUeep9kyrxYR5p9A=s64","userId":"15713361043153944297"}},"outputId":"3afe5c48-face-4c68-8d45-6c24cf6a096d"},"source":["# !pip install pyod\n","\n","from pyod.models.knn import KNN\n","\n","\n","Y = np.sin(50*X)\n","Y = Y.reshape(-1, 1)\n","clf = KNN()\n","clf.fit(Y)\n","outliers = clf.predict(Y)\n","print(outliers)"],"execution_count":132,"outputs":[{"output_type":"stream","text":["[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"," 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"," 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"kufHGEeZ3xVc"},"source":["Isolation Forest explicitly identifies anomalies instead of profiling normal data points. Isolation Forest, like any tree ensemble method, is built on the basis of decision trees. In these trees, partitions are created by first randomly selecting a feature and then selecting a random split value between the minimum and maximum value of the selected feature."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-U-WgRnA3dLG","executionInfo":{"status":"ok","timestamp":1628260151226,"user_tz":-330,"elapsed":632,"user":{"displayName":"Jay Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghp15R1mmnoYYCixHxYvGt-QFWUeep9kyrxYR5p9A=s64","userId":"15713361043153944297"}},"outputId":"ee94396d-b2ae-4635-8b62-05b6e6d1eefd"},"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from pylab import savefig\n","from sklearn.ensemble import IsolationForest\n","# Generating data ----\n","\n","rng = np.random.RandomState(42)\n","\n","# Generating training data \n","X_train = 0.2 * rng.randn(1000, 2)\n","X_train = np.r_[X_train + 3, X_train]\n","X_train = pd.DataFrame(X_train, columns = ['x1', 'x2'])\n","\n","# Generating new, 'normal' observation\n","X_test = 0.2 * rng.randn(200, 2)\n","X_test = np.r_[X_test + 3, X_test]\n","X_test = pd.DataFrame(X_test, columns = ['x1', 'x2'])\n","\n","# Generating outliers\n","X_outliers = rng.uniform(low=-1, high=5, size=(50, 2))\n","X_outliers = pd.DataFrame(X_outliers, columns = ['x1', 'x2'])\n","\n","# Isolation Forest ----\n","\n","# training the model\n","clf = IsolationForest(max_samples=100, random_state=rng)\n","clf.fit(X_train)\n","\n","# predictions\n","y_pred_train = clf.predict(X_train)\n","y_pred_test = clf.predict(X_test)\n","y_pred_outliers = clf.predict(X_outliers)\n","\n","print(y_pred_outliers)"],"execution_count":133,"outputs":[{"output_type":"stream","text":["[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n"," -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n"," -1 -1]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"nyDQ7BFQ4HLy"},"source":["LOF is an unsupervised (well, semi-supervised) machine learning algorithm that uses the density of data points in the distribution as a key factor to detect outliers.\n","LOF compares the density of any given data point to the density of its neighbors. Since outliers come from low-density areas, the ratio will be higher for anomalous data points. As a rule of thumb, a normal data point has a LOF between 1 and 1.5 whereas anomalous observations will have much higher LOF. The higher the LOF the more likely it is an outlier. If the LOF of point X is 5, it means the average density of X’s neighbors is 5 times higher than its local density.\n"]},{"cell_type":"markdown","metadata":{"id":"x1smCOPE4yBO"},"source":["SVM is also increasingly being used in one class problem, where all data belong to a single class. In this case, the algorithm is trained to learn what is “normal”, so that when a new data is shown the algorithm can identify whether it should belong to the group or not. If not, the new data is labeled as out of ordinary or anomaly."]},{"cell_type":"code","metadata":{"id":"LpUNVzyG3dEO"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SN4g1E6M1k8x"},"source":["##If target variable in having only one class , what is your approach ?\n","\n","If the outcome is only one value then algorithm would also return the same value irrespective of anything. if you don't have negative cases then you cannot use Logistic regression and it doesn't know when to say 0 as the model wasn't trained on it.\n","\n","If possible using Business Matter Experts advice populate some dummy data for 0's and balance the data. Train your model and see what the model is going to say.\n","\n","Else you cannot use any kind of classification algorithm."]},{"cell_type":"markdown","metadata":{"id":"1rAOGPFv1k49"},"source":["##Select k value in kmeans clustering\n","\n","**The Elbow Method**\n","\n","Calculate the `Within-Cluster-Sum of Squared` Errors (WSS) for different values of k, and choose the k for which WSS becomes first starts to diminish. In the plot of WSS-versus-k, this is visible as an elbow.\n","\n","**The Silhouette Method**\n","\n","The silhouette value measures how similar a point is to its own cluster (cohesion) compared to other clusters (separation). The range of the Silhouette value is between +1 and -1. A high value is desirable and indicates that the point is placed in the correct cluster.\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"77DqEaJ61kqd"},"source":["##If you want to classify only 2 class in kmeans clustering , k value is 5. What is the approach.\n","\n","We can do classification. The algorithm itself is particularly not well-suited to classifying points, as incorporating data to be classified into your training data tends to be frowned upon.\n","\n","To classify a new point, simply calculate the Euclidean distance to each cluster centroid to determine the closest one, then classify it under that cluster.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"m0PUIBhP1knD"},"source":["##PCA ( very very important ) Explaination , steps (Normalaization,covariance,Eigen values and eigen vectors ), selection of principal components ( EVR , Scree plot )\n","\n","\n","Principal Component Analysis, or PCA is an unsupervised statistical technique used for reducing the dimensionality of data. It can be thought of as a projection method where data with m-columns (features) is projected into a subspace with m or fewer columns, whilst retaining the essence of the original data.\n","\n","PCA tries to put maximum possible information, so that, if you want to reduce your dataset’s dimensionality, you can focus your analysis on the first few components without suffering a great penalty in terms of information loss.\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dEgoHDtUX8nD","executionInfo":{"status":"ok","timestamp":1628251889377,"user_tz":-330,"elapsed":5,"user":{"displayName":"Jay Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghp15R1mmnoYYCixHxYvGt-QFWUeep9kyrxYR5p9A=s64","userId":"15713361043153944297"}},"outputId":"50ee9933-66c3-4c6f-f5ce-19a7ef8087bb"},"source":["from numpy import array\n","from numpy import mean\n","from numpy import cov\n","from numpy.linalg import eig\n","# define a matrix\n","A = array([[1, 2], [3, 4], [5, 6]])\n","print(A)\n","# calculate the mean of each column\n","M = mean(A.T, axis=1)\n","print(M)\n","# center columns by subtracting column means\n","C = A - M\n","print(C)\n","# calculate covariance matrix of centered matrix\n","V = cov(C.T)\n","print(V)\n","# eigendecomposition of covariance matrix\n","values, vectors = eig(V)\n","print(vectors)\n","print(values)\n","# project data\n","P = vectors.T.dot(C.T)\n","print(P.T)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["[[1 2]\n"," [3 4]\n"," [5 6]]\n","[3. 4.]\n","[[-2. -2.]\n"," [ 0.  0.]\n"," [ 2.  2.]]\n","[[4. 4.]\n"," [4. 4.]]\n","[[ 0.70710678 -0.70710678]\n"," [ 0.70710678  0.70710678]]\n","[8. 0.]\n","[[-2.82842712  0.        ]\n"," [ 0.          0.        ]\n"," [ 2.82842712  0.        ]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"DuzdxdSgbN0F"},"source":["###Normalization\n","\n","Normalization is important in PCA since it is a variance maximizing exercise. It projects your original data onto directions which maximize the variance. The first plot below shows the amount of total variance explained in the different principal components wher we have not normalized the data. As you can see, it seems like component one explains most of the variance in the data.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"BSxk84UHbNvs"},"source":["###Covariance Matrix\n","\n","Let’s consider a scenario where we have only two features, x and y. We can represent our data in a 2D graph\n","\n","Now, we can compute what is called Covariance Matrix: it is a symmetric, dxd matrix (where d is the number of features, hence in this case d=2) where the variance of each feature and the cross-features covariances are stored:\n","\n","$$\\sum= \n","\\begin{bmatrix}\n","Var(x) & Cov(x,y)\\\\\n","Cov(y,x) & Var(y)\n","\\end{bmatrix}$$\n","\n","Since Cov(x,y) is equal to Cov(y,x), the matrix is, as said, symmetric, and the variances of the features lie on the principal diagonal.\n","The covariance matrix can assume different values depending on the shape of our data. \n","\n","\n","When the two features are positively correlated, the covariance is greater than zero, otherwise, it has a negative value. Furthermore, if there is no evidence of a correlation between them, hence the covariance is equal to zero."]},{"cell_type":"markdown","metadata":{"id":"wYZeweiaYmQV"},"source":["###Eigenvectors and Eigenvalues\n","\n"," let’s consider the following situation. We are provided with 2-dimensional vectors v1, v2, …, vn. Then, if we apply a linear transformation T (a 2x2 matrix) to our vectors, we will obtain new vectors, called b1, b2,…,bn.\n","\n","$$Tv_1=b1 \\\\\n","Tv_2=b2 \\\\\n",".... \\\\\n","Tv_n=b_n$$\n","\n","\n","Some of them (more specifically, as many as the number of features), though, have a very interesting property: indeed, once applied the transformation T, they change length but not direction. Those vectors are called eigenvectors, and the scalar which represents the multiple of the eigenvector is called eigenvalue.\n","\n","$$Tv_i=\\lambda v_i$$\n","\n","Thus, each eigenvector has a correspondent eigenvalue. Now, if we consider our matrix Σ and collect all the corresponding eigenvectors into a matrix V (where the number of columns, which are the eigenvectors, will be equal to the number of rows of Σ), we will obtain something like that:\n","\n","$$\\sum V= LV$$\n","\n","Where L a vector where all the eigenvalues (as many as the eigenvectors) are stored. If we consider our example of two features (x and y), we will obtain the following:\n","\n","$$\n","\\begin{equation}\n","\\begin{bmatrix}\n","Var(x) & Cov(x,y)\\\\\n","Cov(y,x) & Var(y)\n","\\end{bmatrix}\n","\\begin{bmatrix}\n","v_1\\\\\n","v_2\n","\\end{bmatrix}'=\n","\\begin{bmatrix}\n","\\lambda_1\\\\\n","\\lambda_2\n","\\end{bmatrix}\n","\\begin{bmatrix}\n","v_1\\\\\n","v_2\n","\\end{bmatrix}\n","\\end{equation}\n","$$\n","\n","Then, if we sort our eigenvectors in descending order with respect to their eigenvalues, we will have that the first eigenvector accounts for the largest spread among data, the second one for the second largest spread and so forth.\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ruOlkVS8YmNk"},"source":["###selection of principal components\n","\n","**Explained Variance Ratio**\n","\n"," It represents the amount of variance each principal component is able to explain.\n","\n","For example, suppose if the square of distances of all the points from the origin that lie on PC1 is 50 and for the points on PC2 it’s 5.\n","\n","$$EVR \\ of \\ PC1= \\frac{DistanceofPC1points}{(DistanceofPC1points+DistanceofPC2points)}=\\frac{50}{55}=0.91%%$$\n","\n","$$EVR \\ of \\ PC2= \\frac{DistanceofPC2points}{(DistanceofPC1points+DistanceofPC2points)}=\\frac{5}{55}=0.09%%$$\n","\n","\n","Thus PC1 explains 91% of the variance of data. Whereas, PC2 only explains 9% of the variance. Hence we can use only PC1 as the input for our model as it explains the majority of the variance.\n","\n","\n","**Scree Plots:**\n","\n","In multivariate statistics, a scree plot is a line plot of the eigenvalues of factors or principal components in an analysis. The scree plot is used to determine the number of factors to retain in an exploratory factor analysis (FA) or principal components to keep in a principal component analysis (PCA).\n","\n","Below is the example of Scree plot:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":350},"id":"pEbLRoznd6z6","executionInfo":{"status":"ok","timestamp":1628253786915,"user_tz":-330,"elapsed":485,"user":{"displayName":"Jay Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghp15R1mmnoYYCixHxYvGt-QFWUeep9kyrxYR5p9A=s64","userId":"15713361043153944297"}},"outputId":"a60f5402-52df-4b95-c507-fff4e5342832"},"source":["import numpy as np\n","import matplotlib\n","import matplotlib.pyplot as plt\n","\n","# Random array and then make it positive-definite\n","num_vars = 6\n","num_obs = 9\n","A = np.random.randn(num_obs, num_vars)\n","A = np.asmatrix(A.T) * np.asmatrix(A)\n","U, S, V = np.linalg.svd(A) \n","eigvals = S**2 / np.sum(S**2) \n","\n","fig = plt.figure(figsize=(8,5))\n","sing_vals = np.arange(num_vars) + 1\n","plt.plot(sing_vals, eigvals, 'ro-', linewidth=2)\n","plt.title('Scree Plot')\n","plt.xlabel('Principal Component')\n","plt.ylabel('Eigenvalue')\n","leg = plt.legend(['Eigenvalues from SVD'], loc='best', borderpad=0.3, \n","                 shadow=False, prop=matplotlib.font_manager.FontProperties(size='small'),\n","                 markerscale=0.4)\n","leg.get_frame().set_alpha(0.4)\n","plt.show()"],"execution_count":7,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 576x360 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"ZfGye1cS1kka"},"source":["##DBSCAN\n","\n","\n","DBSCAN (Density-Based Spatial Clustering of Applications with Noise) is a popular learning method utilized in model building and machine learning algorithms. This is a clustering method that is used in machine learning to separate clusters of high density from clusters of low density. \n","\n","It works on the assumption that clusters are dense regions in space separated by regions of lower density. It groups 'densely grouped' data points into a single cluster."]},{"cell_type":"markdown","metadata":{"id":"bjKJJkBC1khD"},"source":["##Evaluation of clustering\n","\n","Three important factors by which clustering can be evaluated are\n","\n","(a) Clustering tendency (b) Number of clusters, k (c) Clustering quality\n","\n","\n","**Clustering tendency**\n","\n","Before evaluating the clustering performance, making sure that data set we are working has clustering tendency and does not contain uniformly distributed points is very important. If the data does not contain clustering tendency, then clusters identified by any state of the art clustering algorithms may be irrelevant. Non-uniform distribution of points in data set becomes important in clustering.\n","To solve this, Hopkins test, a statistical test for spatial randomness of a variable, can be used to measure the probability of data points generated by uniform data distribution.\n","\n","`Null Hypothesis (Ho) :` Data points are generated by uniform distribution (implying no meaningful clusters)\n","`Alternate Hypothesis (Ha) :` Data points are generated by random data points (presence of clusters)\n","\n","\n","**Number of Optimal Clusters, k**\n","\n","Some of the clustering algorithms like K-means, require number of clusters, k, as clustering parameter. Getting the optimal number of clusters is very significant in the analysis. If k is too high, each point will broadly start representing a cluster and if k is too low, then data points are incorrectly clustered. Finding the optimal number of clusters leads to granularity in clustering.\n","\n","\n","**Clustering quality**\n","\n","Once clustering is done, how well the clustering has performed can be quantified by a number of metrics. Ideal clustering is characterised by minimal intra cluster distance and maximal inter cluster distance.\n","There are majorly two types of measures to assess the clustering performance.\n","1. Extrinsic Measures which require ground truth labels. Examples are Adjusted Rand index, Fowlkes-Mallows scores, Mutual information based scores, Homogeneity, Completeness and V-measure.\n","2. Intrinsic Measures that does not require ground truth labels. Some of the clustering performance measures are Silhouette Coefficient, Calinski-Harabasz Index, Davies-Bouldin Index etc.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"A3sQL12gxbAa"},"source":["##Different algorithms in clustering\n","\n","Clustering techniques apply when there is no class to be predicted but rather when the instances are to be divided into natural groups.\n","\n","There are many types of clustering algorithms.\n","\n","Many algorithms use similarity or distance measures between examples in the feature space. Some clustering algorithms require you to specify or guess at the number of clusters to discover in the data, whereas others require the specification of some minimum distance between observations in which examples may be considered “close” or “connected.”\n","\n","Few more popular algorithms is as follows:\n","\n","* Affinity Propagation\n","* Agglomerative Clustering\n","* BIRCH\n","* DBSCAN\n","* K-Means\n","* Mini-Batch K-Means\n","* Mean Shift\n","* OPTICS\n","* Spectral Clustering\n","* Mixture of Gaussians\n","\n","**Affinity Propogation**\n","\n","We devised a method called “affinity propagation,” which takes as input measures of similarity between pairs of data points. Real-valued messages are exchanged between data points until a high-quality set of exemplars and corresponding clusters gradually emerges\n","\n","\n","**Agglomerative Clustering**\n","\n","Agglomerative clustering involves merging examples until the desired number of clusters is achieved.\n","\n","**BIRCH**\n","\n","BIRCH Clustering (BIRCH is short for Balanced Iterative Reducing and Clustering using\n","Hierarchies) involves constructing a tree structure from which cluster centroids are extracted.\n","\n","**DBSCAN**\n","\n","DBSCAN Clustering (where DBSCAN is short for Density-Based Spatial Clustering of Applications with Noise) involves finding high-density areas in the domain and expanding those areas of the feature space around them as clusters.\n","\n","**K-Means**\n","\n","K-Means Clustering may be the most widely known clustering algorithm and involves assigning examples to clusters in an effort to minimize the variance within each cluster.\n","\n","**Mini-Batch K-Means**\n","\n","Mini-Batch K-Means is a modified version of k-means that makes updates to the cluster centroids using mini-batches of samples rather than the entire dataset, which can make it faster for large datasets, and perhaps more robust to statistical noise.\n","\n","**Mean Shift**\n","\n","Mean shift clustering involves finding and adapting centroids based on the density of examples in the feature space.\n","\n","**OPTICS**\n","\n","OPTICS clustering (where OPTICS is short for Ordering Points To Identify the Clustering Structure) is a modified version of DBSCAN described above.\n","\n","**Spectral Clustering**\n","\n","Spectral Clustering is a general class of clustering methods, drawn from linear algebra. A promising alternative that has recently emerged in a number of fields is to use spectral methods for clustering. Here, one uses the top eigenvectors of a matrix derived from the distance between points.\n","\n","\n","**Gaussian Mixture Model**\n","\n","A Gaussian mixture model summarizes a multivariate probability density function with a mixture of Gaussian probability distributions as its name suggests.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"7it-UPTFxa94"},"source":["##How do u choose the suitable clustering approach for the given sets of data ?\n","\n","\n","For choosing cluster there are direct methods and statistical testing methods:\n","\n","1. Direct methods: consists of optimizing a criterion, such as the within cluster sums of squares or the average silhouette. The corresponding methods are named elbow and silhouette methods, respectively.\n","2. Statistical testing methods: consists of comparing evidence against null hypothesis. An example is the gap statistic.\n","\n","**Elbow method**\n","\n","The basic idea behind partitioning methods, in k-means clustering, is to define clusters such that the total intra-cluster variation (or total within-cluster sum of square (WSS)) is minimized. The total WSS measures the compactness of the clustering and we want it to be as small as possible.\n","\n","The Elbow method looks at the total WSS as a function of the number of clusters: One should choose a number of clusters so that adding another cluster doesn’t improve much better the total WSS.\n","\n","The optimal number of clusters can be defined as follow:\n","\n","1. Compute clustering algorithm (e.g., k-means clustering) for different values of k. For instance, by varying k from 1 to 10 clusters.\n","2. For each k, calculate the total within-cluster sum of square (wss).\n","3. Plot the curve of wss according to the number of clusters k.\n","4. The location of a bend (knee) in the plot is generally considered as an indicator of the appropriate number of clusters.\n","\n","\n","\n","**Average silhouette method**\n","\n","It measures the quality of a clustering. That is, it determines how well each object lies within its cluster. A high average silhouette width indicates a good clustering.\n","\n","Average silhouette method computes the average silhouette of observations for different values of k. The optimal number of clusters k is the one that maximize the average silhouette over a range of possible values for k \n","\n","The algorithm is similar to the elbow method and can be computed as follow:\n","\n","1. Compute clustering algorithm (e.g., k-means clustering) for different values of k. For instance, by varying k from 1 to 10 clusters.\n","2. For each k, calculate the average silhouette of observations (avg.sil).\n","3. Plot the curve of avg.sil according to the number of clusters k.\n","4. The location of the maximum is considered as the appropriate number of clusters.\n","\n","**Gap statistic method**\n","\n","The gap statistic compares the total within intra-cluster variation for different values of k with their expected values under null reference distribution of the data. The estimate of the optimal clusters will be value that maximize the gap statistic (i.e, that yields the largest gap statistic). This means that the clustering structure is far away from the random uniform distribution of points.\n","\n","The algorithm works as follow:\n","\n","1. Cluster the observed data, varying the number of clusters from `k = 1, …, kmax`, and compute the corresponding total within intra-cluster variation Wk.\n","2. Generate B reference data sets with a random uniform distribution. Cluster each of these reference data sets with varying number of clusters `k = 1, …, kmax`, and compute the corresponding total within intra-cluster variation Wkb.\n","3. Compute the estimated gap statistic as the deviation of the observed Wk value from its expected value Wkb under the null hypothesis: $Gap(k)=1B∑b=1Blog(W∗kb)−log(Wk)$. Compute also the standard deviation of the statistics.\n","4. Choose the number of clusters as the smallest value of k such that the gap statistic is within one standard deviation of the gap at $k+1: Gap(k)≥Gap(k + 1)−sk + 1$."]},{"cell_type":"markdown","metadata":{"id":"HtZMJI1gxa7-"},"source":["##Decision tree and its advantages\n","\n","\n","Decision Tree solves the problem of machine learning by transforming the data into a tree representation. Each internal node of the tree representation denotes an attribute and each leaf node denotes a class label.\n","A decision tree algorithm can be used to solve both regression and classification problems.\n","\n","Decision Trees are a type of Supervised Machine Learning where the data is continuously split according to a certain parameter. The tree can be explained by two entities, namely decision nodes and leaves.\n","\n","Advantages:\n","* Compared to other algorithms decision trees requires less effort for data preparation during pre-processing.\n","* A decision tree does not require normalization of data.\n","* A decision tree does not require scaling of data as well.\n","* Missing values in the data also do NOT affect the process of building a decision tree to any considerable extent.\n","* A Decision tree model is very intuitive and easy to explain to technical teams as well as stakeholders."]},{"cell_type":"markdown","metadata":{"id":"5ejza4r1xa6B"},"source":["##Entropy and Information gain\n","\n","\n","\n","Entropy provides an absolute limit on the shortest possible average length of a lossless compression encoding of the data produced by a source, and if the entropy of the source is less than the channel capacity of the communication channel,the data generated by the source can be reliably communicated to the receiver.\n","\n","\n","$$Entropy= \\sum_{i} P(i).log_{2}P(i)$$\n","\n","\n","**Information Gain**\n","\n","Now that we have discussed Entropy we can move forward into information gain. This is the concept of a decrease in entropy after splitting the data on a feature. The greater the information gain, the greater the decrease in entropy or uncertainty.\n","\n","$$InformationGain(T,X)= Entropy(T)- \\sum_{splits}\\frac{s_1}{T}Entropy(s_1)$$\n","\n","* T: Target population prior to the split T=∑ {All Splits}, the total number of observation before splitting.\n","* Entropy(T): Measure the disorder before the split,or level of uncertainty\n","* s{i}: is the number of observations on the i{th} split\n","* Entropy(s{i}): Measures the disorder for the target variable on split s{i}"]},{"cell_type":"markdown","metadata":{"id":"6D9RFPGExa08"},"source":["##Gini impurity\n","\n","\n","Gini Impurity is a measurement used to build Decision Trees to determine how the features of a dataset should split nodes to form the tree. More precisely, the Gini Impurity of a dataset is a number between 0-0.5, which indicates the likelihood of new, random data being misclassified if it were given a random class label according to the class distribution in the dataset.\n","\n","Mathematical definition:\n","\n","Consider a dataset D that contains samples from k classes. The probability of samples belonging to class i at a given node can be denoted as pi. Then the Gini Impurity of D is defined as:\n","$$Gini(D)= 1- \\sum_{i-1}^{k}p_i^2$$\n","\n","Gini works only in those scenarios where we have categorical targets. It does not work with continuous targets.\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":299},"id":"0I5sr66x4iTR","executionInfo":{"status":"ok","timestamp":1628226926023,"user_tz":-330,"elapsed":1156,"user":{"displayName":"Jay Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghp15R1mmnoYYCixHxYvGt-QFWUeep9kyrxYR5p9A=s64","userId":"15713361043153944297"}},"outputId":"92fbfd52-9bf1-416b-d115-8a0e4df5d731"},"source":["# Visualizing Gini Impurity range\n","\n","import numpy as np\n","import pandas as pd\n","import os\n","import matplotlib.pyplot as plt\n","\n","plt.figure()\n","x = np.linspace(0.01,1)\n","y = 1 - (x*x) - (1-x)*(1-x) \n","plt.plot(x,y)\n","plt.title('Gini Impurity')\n","plt.xlabel(\"Fraction of Class k ($p_k$)\")\n","plt.ylabel(\"Impurity Measure\")\n","plt.xticks(np.arange(0,1.1,0.1))\n","\n","plt.show()"],"execution_count":1,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"A-joMVDExayX"},"source":["##Decision tree hyperparameters\n","\n","\n","* `criterion: string, optional (default=”gini”):`\n","\n","The function to measure the quality of a split. Supported criteria are “gini” for the Gini impurity and “entropy” for the information gain.\n","\n","* `splitter: string, optional (default=”best”)`\n","\n","The strategy used to choose the split at each node. Supported strategies are “best” to choose the best split and “random” to choose the best random split.\n","\n","* `max_depth: int or None, optional (default=None)`\n","\n","The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.\n","\n","* `min_samples_split: int, float, optional (default=2)`\n","\n","The minimum number of samples required to split an internal node:\n","\n","* `min_samples_leaf: int, float, optional (default=1)`\n","\n","The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least min_samples_leaf training samples in each of the left and right branches. This may have the effect of smoothing the model, especially in regression.\n","\n","* `min_weight_fraction_leaf: float, optional (default=0.)`\n","\n","The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample_weight is not provided.\n","\n","\n","* `max_features: int, float, string or None, optional (default=None)`\n","\n","The number of features to consider when looking for the best split:\n","\n","\n","* `min_impurity_decrease: float, optional (default=0.)`\n","\n","A node will be split if this split induces a decrease of the impurity greater than or equal to this value.\n","\n","The weighted impurity decrease equation is the following:\n","```bash\n","N_t / N * (impurity - N_t_R / N_t * right_impurity \n","                    - N_t_L / N_t * left_impurity)\n","```\n","\n","* `class_weight: dict, list of dicts, “balanced” or None, default=None`\n","\n","Weights associated with classes in the form {class_label: weight}. If not given, all classes are supposed to have weight one. For multi-output problems, a list of dicts can be provided in the same order as the columns of y.\n","\n","\n","* `presort: bool, optional (default=False)`\n","\n","Whether to presort the data to speed up the finding of best splits in fitting. For the default settings of a decision tree on large datasets, setting this to true may slow down the training process. When using either a smaller dataset or a restricted depth, this may speed up the training.\n"]},{"cell_type":"markdown","metadata":{"id":"Fpop1UK1xavl"},"source":["##Pre and post pruning -handle the overfitting\n","\n","Pruning is a process of removal of selected part of plant such as bud,branches and roots . In Decision Tree pruning does the same task it removes the branchesof decision tree to overcome the overfitting condition of decision tree.\n","This can be done in two ways:\n","\n","1. Post Pruning :\n","\n","  * This technique is used after construction of decision tree.\n","  * This technique is used when decision tree will have very large depth and will show overfitting of model.\n","  * It is also known as backward pruning.\n","  * This technique is used when we have infinitely grown decision tree.\n","  * Here we will control the branches of decision tree that is max_depth and `min_samples_split` using `cost_complexity_pruning`\n","\n","2. Pre-Pruning :\n","  * This technique is used before construction of decision tree.\n","  * Pre-Pruning can be done using Hyperparameter tuning.\n","  * Overcome the overfitting issue.\n","  * We can use GridSearchCV for Hyperparameter tuning.\n"]},{"cell_type":"markdown","metadata":{"id":"NeuE8y0uxasd"},"source":["##Random forest and XG Boost\n","\n","\n","**Random Forest Algorithm**\n","\n","Random Forest is an ensemble technique that is a tree-based algorithm. The process of fitting no decision trees on different subsample and then taking out the average to increase the performance of the model is called “Random Forest”. \n","It is preferred over all other algorithms because of its ability to give high accuracy and to prevent overfitting by making use of more trees. There are several different hyperparameters like no trees, depth of trees, jobs, etc in this algorithm. \n","\n","```python\n","from sklearn.ensemble import RandomForestClassifier\n","\n","rfcl = RandomForestClassifier()\n","```\n","\n","\n","**XG Boost**\n","\n","XGBoost is termed as Extreme Gradient Boosting Algorithm which is again an ensemble method that works by boosting trees. XGboost makes use of a gradient descent algorithm which is the reason that it is called Gradient Boosting. The whole idea is to correct the previous mistake done by the model, learn from it and its next step improves the performance. The previous results are rectified and performance is enhanced.\n","\n","This gets continued until there is no scope of further improvements. Regularization is the feature that is dominant for this type of predictive algorithm. It is fast to execute and gives good accuracy. \n","\n","```python\n","from xgboost import XGBClassifier\n","\n","xgbcl = XGBClassifier()\n","```\n","\n"]},{"cell_type":"markdown","metadata":{"id":"e4Tf-7-b1keQ"},"source":["##XG Boost equations\n","\n","XGBoost uses loss function to build trees by minimizing the following value:\n","\n","\n","$$obj= \\sum_{i=1}^{n}l(y_i,y_i'^{(t)}) + \\sum_{i=1}^{t}\\Omega(f_i)$$\n","$$Where \\  \\Omega(f) = \\gamma T+ \\frac{1}{2}\\lambda||\\omega||^2$$\n","\n","In this equation, the first part represents for loss function which calculates the pseudo residuals of predicted value $y_i'$ and true value yi in each leaf, the second part contains two parts just showed as above. The last part in omega formula contains regularization term lambda which intended to reduce the prediction’s insensitivity to individual observations and w represents the leaf weights which we could also consider it as the output value for the leaf. Also, T represents the number of terminal nodes or leaves in a tree and gamma represents the user-definable penalty which meant to encourage pruning.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"jlcDjaj61kbU"},"source":["##Confusion matrix\n","\n","\n","A confusion matrix is a technique for summarizing the performance of a classification algorithm.\n","\n","Classification accuracy alone can be misleading if you have an unequal number of observations in each class or if you have more than two classes in your dataset.\n","\n","Calculating a confusion matrix can give you a better idea of what your classification model is getting right and what types of errors it is making.\n","\n","It gives us:\n","\n","* `true positive` for correctly predicted event values.\n","* `false positive` for incorrectly predicted event values.\n","* `true negative` for correctly predicted no-event values.\n","* `false negative` for incorrectly predicted no-event values.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rvfyPOrNCYlU","executionInfo":{"status":"ok","timestamp":1628229442440,"user_tz":-330,"elapsed":1090,"user":{"displayName":"Jay Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghp15R1mmnoYYCixHxYvGt-QFWUeep9kyrxYR5p9A=s64","userId":"15713361043153944297"}},"outputId":"7862f931-fc01-47e9-ea85-0477cfb20724"},"source":["from sklearn.metrics import confusion_matrix\n"," \n","expected = [1, 1, 0, 1, 0, 0, 1, 0, 0, 0]\n","predicted = [1, 0, 0, 1, 0, 0, 1, 1, 1, 0]\n","results = confusion_matrix(expected, predicted)\n","print(results)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["[[4 2]\n"," [1 3]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qy5h-BUa1kZ0"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"L2xAuHjg1kUx"},"source":["##Precision and recall in layman way\n","\n","**Precision**\n","\n","Precision is a metric that quantifies the number of correct positive predictions made.\n","\n","Precision, therefore, calculates the accuracy for the minority class.\n","\n","It is calculated as the ratio of correctly predicted positive examples divided by the total number of positive examples that were predicted.\n","\n","`For example` if we are predicting heart disease then it would measure of patients that have correctly identify having a heart disease out of all the patients actually having it.\n","\n","\n","$$Precision = \\frac{TruePositives}{(TruePositives + FalsePositives)}$$\n","\n","**Recall**\n","\n","The recall is the measure of our model correctly identifying True Positives. \n","\n","Thus, for all the patients who actually have heart disease, recall tells us how many we correctly identified as having a heart disease.\n","\n","Mathematically:\n","\n","$$Recall = \\frac{TruePositives}{(TruePositives + FalseNegative)}$$"]},{"cell_type":"markdown","metadata":{"id":"PayKzWpdClUv"},"source":["##Use cases for precision and recall\n","\n","You may decide to use precision or recall on your imbalanced classification problem.\n","\n","Maximizing precision will minimize the number false positives, whereas maximizing the recall will minimize the number of false negatives.\n","\n","* `Precision:` Appropriate when minimizing false positives is the focus.\n","\n","For example, precision is good to use if you are a restaurant owner looking to buy wine for your restaurant only if it is predicted to be good by a classifier algorithm.\n","\n","* `Recall:` Appropriate when minimizing false negatives is the focus.\n","\n","For example, you should use recall when looking to predict wether a credit card charge is fraudulent or not. If you have a lot of false negatives, then you have a lot of fraudulent charges that are being labeled as not fraudulent and customers will have money stolen from them.\n"]},{"cell_type":"markdown","metadata":{"id":"hsuEuVK0ClRz"},"source":["##What u do if u get 90% accuracy for the model ?\n","\n","You can fine tune classification model for gaining higher accuracy. U can do following to gain more accuracy—\n","\n","1. Missing value analysis and imputation\n","2. Feature engineering\n","3. Normalisation and standardisation\n","4. Cross validation measures\n","5. Fine-tuning hyper parameters boosting Algorithms\n","6. Ensambling\n","7. Collecting more data\n","8. Synthesizing more data"]},{"cell_type":"markdown","metadata":{"id":"euVg9lUCClOp"},"source":["##What u do if u get 98% accuracy for the model ?\n","\n","\n","The odds are great that your model is overfitting but there’s a tiny chance that the model is extremely good.\n","\n","That’s why you need to run the model on a separate test/validation set so that you will know which is the case.\n","\n","Reason why your model is probably overfitting:\n","\n","* Deep Learning (DL) is applied to complex problems (like computer vision.). Complex problems almost always have a much worse accuracy. You could get this high an accuracy on a trivial problem but that would be pointless, although if you’re asking this question, you might indeed be using DL on a trivial problem.\n","* The odds are you haven’t gone through a lot of iterations of the model. Your first model is likely to be relatively poor. You’ll need to try different number of hidden layers, etc. If you achieve 98–100% accuracy after many trials, you’d know how much work it takes to get to such a high level of accuracy.\n","* The odds are your training set isn’t large enough. Too small a training set, and you’ll overfit.\n","* If the problem is complex enough, the odds are your data will have bad labels in it. That will cause the accuracy to go down. If you don’t have any bad labels, then you’re probably applying DL to a simpler problem, which doesn’t warrant DL."]},{"cell_type":"markdown","metadata":{"id":"V3Wy9NDFClL0"},"source":["##Steps involved in deployment of the model\n","\n","There are different approaches to putting models into productions, with benefits that can vary dependent on the specific use case.\n","\n","There is generally different ways to both train and server models into production:\n","\n","* Train: one off, batch and real-time/online training\n","* Serve: Batch, Realtime (Database Trigger, Pub/Sub, web-service, inApp)\n","\n","\n","`Model Format`\n","\n","Pickle converts a python object to to a bitstream and allows it to be stored to disk and reloaded at a later time. It is provides a good format to store machine learning models provided that their intended applications is also built in python.\n","\n","`Training`\n","\n","For one off training of models, the model can either be trained and fine tune ad hoc by a data-scientists or training through AutoML libraries. Having an easily reproducible setup, however helps pushing into the next stage of productionalization, ie: batch training.\n","\n","`Batch Training`\n","\n","While not fully necessary to implement a model in production, batch training allows to have a constantly refreshed version of your model based on the latest train.\n","\n","`Real time training`\n","\n","Real-time training is possible with ‘Online Machine Learning’ models, algorithms supporting this method of training includes K-means (through mini-batch), Linear and Logistic Regression (through Stochastic Gradient Descent) as well as Naive Bayes classifier.\n","\n","`Batch vs. Real-time Prediction`\n","\n","\n","When looking at whether to setup a batch or real-time prediction, it is important to get an understanding of why doing real-time prediction would be important.\n","\n","`Batch Prediction Integration`\n","\n","Batch predictions rely on two different set of information, one is the predictive model and the other one is the features that we will feed the model. In most type of batch prediction architecture, ETL is performed to either fetch pre-calculated features from a specific datastore (feature-store) or performing some type of transformation across multiple datasets to provide the input to the prediction model.\n","\n","\n","`Real-time Prediction integration`\n","\n","Being able to push model into production for real-time applications require 3 base components. A customer/user profile, a set of triggers and predictive models.\n","\n","`Database integrations`\n","\n","If the overall size of your database is fairly small (< 1M user profile) and the update frequency is occasional it can make sense to integrate some of the real-time update process directly within the database.\n","\n","\n","\n","\n","`Containerize Your Code`\n","\n","Docker is by far the most popular way developers containerized their code. \n","\n","\n","`Deploy to the Cloud`\n","\n","If all of your work has been on your local machine. If things have gone well you have a front-end web app running on your machine that allows you to access your machine learning model predictions.\n","\n","The three main cloud providers are AWS, Google Cloud, and Microsoft Azure. \n"]},{"cell_type":"markdown","metadata":{"id":"TPG8L9lu3p91"},"source":["##Which cloud did u use ?\n","\n","I have used AWS cloud service provider. There I have used many services like \n","* EC2 instance for running application, \n","* S3 bucket for storing data, \n","* IAM for security reason, \n","* ECR and ECS for storing running and managing containers"]},{"cell_type":"markdown","metadata":{"id":"RYz74MgE3p7A"},"source":["##What u do if the model is not performed/model drift after certain period ?\n","\n","`Lack of Monitoring:`\n","The model in production needs to be monitored on regular basis. The data might change with time, the model that was performing well earlier, the performance will decrease with time. The response variable or the independent variable might change over time that may impact the predictors.\n","\n","`Bias Variance Tradeoff:`\n","The bias-variance problem is the conflict in trying to simultaneously minimize these two sources of error that prevent supervised machine learning algorithms from generalizing beyond their training set.\n","A model having high bias and low variance assumes more assumptions about the form of the target function, and a model having high variance and low bias over learns the training dataset.\n","\n","`Un-representative sampling:`\n","In many cases, we end up training models on a population that is significantly different from the actual population.\n","\n","`Unstable Models:`\n","Some of the models are often highly unstable and do not perform that well with time. In such cases, the business might demand high-frequency model revision and model monitoring. With higher lead time in model creation, businesses might start going back to intuition-based strategy.\n","\n","`Model dependent on highly dynamic variables:`\n","Dynamic variables are those that change a lot with time. When a model is highly dependent on such dynamic variables and it brings a lot of predicting power to the model hence increasing the performance. Due to a change in these dynamic variables, the model performance is affected to a great extent. \n","\n","`Training too complex models:`\n","The predictive power of an ML model is the soul of the problem solution. But, predictive power comes at a cost of the complexity of the model. More complex ensemble models have better performance compared to a simple model, but the interpretability of the model decreases. Such models might be amazing in performance, but once deployed in production the performance starts degrading.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Vv-YVFKWC3Bw"},"source":["##Where do u store the data ?\n","\n","We used Relational databases for storing and accessing very large datasets. like: MySQL and Postgres.\n","\n","For storing model we use S3 bucket."]},{"cell_type":"markdown","metadata":{"id":"0yexazgbC3AP"},"source":["##How do u handle the continuous flow of data ?\n","\n","We can handle the continuous flow of data using Data Streaming.\n","Data streaming is the process of sending data records continuously rather than in batches. Generally, data streaming is useful for the types of data sources that send data in small sizes in a continuous flow as the data is generated.\n","\n","Few popular tools for working with streaming data:\n","\n","* `Amazon Kinesis Firehose.` Amazon Kinesis is a managed, scalable, cloud-based service which allows real-time processing of large data streams.\n","* `Apache Kafka.` Apache Kafka is a distributed publish-subscribe messaging system which integrates applications and data streams.\n","* `Apache Flink.` Apache Flink is a streaming data flow engine which provides facilities for distributed computation over data streams.\n","* `Apache Storm.` Apache Storm is a distributed real-time computation system. Storm is used for distributed machine learning, real-time analytics, and numerous other cases, especially with high data velocity."]},{"cell_type":"markdown","metadata":{"id":"EtCJpa7-C2rw"},"source":["##Time series forecasting - ARIMA \n","\n","\n","A time series is a sequence where a metric is recorded over regular time intervals.\n","Forecasting is the next step where you want to predict the future values the series is going to take.\n","\n","ARIMA, short for `Auto Regressive Integrated Moving Average` is actually a class of models that ‘explains’ a given time series based on its own past values, that is, its own lags and the lagged forecast errors, so that equation can be used to forecast future values.\n","\n","An ARIMA model is characterized by 3 terms: p, d, q\n","\n","where,\n","\n","* p is the order of the AR term\n","\n","* q is the order of the MA term\n","\n","* d is the number of differencing required to make the time series stationary\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"glHFOE1tC2oa"},"source":["##Pacf and acf plot to choose p and q\n","\n","\n","**ACF** is an (complete) auto-correlation function which gives us values of auto-correlation of any series with its lagged values. We plot these values along with the confidence band and tada. ACF plot for most optimal in the MA(q) model.\n","\n","Moving average (MA) process, a process where the present value of series is defined as a linear combination of past errors. We assume the errors to be independently distributed with the normal distribution. The MA process of order q is defined as ,\n","\n","$$y_t= c+ \\epsilon_t +\\theta_{1 \\epsilon_{t-1}} + \\theta_{1 \\epsilon_{t-2}}+...+\\theta_{q \\epsilon_{t-q}}$$\n","\n","Here ϵt is a white noise. To get intuition of MA process lets consider order 1 MA process which will look like,\n","$$y_t= c+ \\epsilon_t + \\theta_{1 \\epsilon_{t-1}}$$\n","\n","\n","\n","**PACF** is a partial auto-correlation function. Basically instead of finding correlations of present with lags like ACF, it finds correlation of the residuals with the next lag value hence ‘partial’ and not ‘complete’ as we remove already found variations before we find the next correlation. PACF plot for most optimal in the AR(p) model.\n","\n","Auto regressive (AR) process , a time series is said to be AR when present value of the time series can be obtained using previous values of the same time series i.e the present value is weighted average of its past values. Stock prices and global temperature rise can be thought of as an AR processes.\n","\n","The AR process of an order p can be written as,\n","\n","$$y_i= c+ \\Phi_1y_{y-1} + \\Phi_2y_{t-2}+...+\\Phi_py_{t-p}+ \\epsilon_t$$\n","\n","Where ϵt is a white noise and y’t-₁ and y’t-₂ are the lags. Order p is the lag value after which PACF plot crosses the upper confidence interval for the first time. These p lags will act as our features while forecasting the AR time series. \n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"uGX2s3VCC2lu"},"source":["#Deep learning and CV\n","\n","##What is neural network?\n","\n","A neural network is a series of algorithms that endeavors to recognize underlying relationships in a set of data through a process that mimics the way the human brain operates. In this sense, neural networks refer to systems of neurons, either organic or artificial in nature.\n","\n","A neural network has many layers. Each layer performs a specific function, and the complex the network is, the more the layers are. That’s why a neural network is also called a multi-layer perceptron.\n","\n","The purest form of a neural network has three layers:\n","\n","1. The input layer\n","2. The hidden layer\n","3. The output layer\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"hZqWKgpy3p4d"},"source":["##What is activation functions ? what is use of Activation functions ?\n","\n","An activation function in a neural network defines how the weighted sum of the input is transformed into an output from a node or nodes in a layer of the network. Activation function decides whether the neuron should be activated or notby calculating weighted sum and further adding bias with it. \n","\n","The purpose of the activation function is to introduce non-linearity into the output of a neuron. It is a function that is added into an artificial neural network in order to help the network learn complex patterns in the data. When comparing with a neuron-based model that is in our brains, the activation function is at the end deciding what is to be fired to the next neuron.\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"yjQdAwZ3Xti7"},"source":["##Gradient descent\n","\n","Gradient descent is an optimization algorithm used to find the values of parameters (coefficients) of a function (f) that minimizes a cost function (cost).\n","\n","Gradient descent is best used when the parameters cannot be calculated analytically (e.g. using linear algebra) and must be searched for by an optimization algorithm.\n","\n","The Gradient descent algorithm multiplies the gradient by a number (Learning rate or Step size) to determine the next point. For example: having a gradient with a magnitude of 4.2 and a learning rate of 0.01, then the gradient descent algorithm will pick the next point 0.042 away from the previous point.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"cAU1z5bzXtWN"},"source":["##Back propagation works \n","\n","Back-propagation is just a way of propagating the total loss back into the neural network to know how much of the loss every node is responsible for, and subsequently updating the weights in such a way that minimizes the loss by giving the nodes with higher error rates lower weights and vice versa.\n","\n","\n","To update the weight, we calculate the error correspond to each weight with the help of a total error. The error on weight w is calculated by differentiating total error with respect to w.\n","\n","$$Error_w= \\frac{\\partial E_{total}}{\\partial w}$$\n","\n"]},{"cell_type":"markdown","metadata":{"id":"FowfmxrDXtH7"},"source":["##Vanishing and exploring gradient descent \n","\n","**Vanishing:**\n","As the backpropagation algorithm advances downwards(or backward) from the output layer towards the input layer, the gradients often get smaller and smaller and approach zero which eventually leaves the weights of the initial or lower layers nearly unchanged. As a result, the gradient descent never converges to the optimum. This is known as the vanishing gradients problem.\n","\n","The model weights may become 0 during training.\n","\n","\n","**Exploding:**\n","\n","On the contrary, in some cases, the gradients keep on getting larger and larger as the backpropagation algorithm progresses. This, in turn, causes very large weight updates and causes the gradient descent to diverge. This is known as the exploding gradients problem.\n","\n","The model weights may become NaN during training.\n"]},{"cell_type":"markdown","metadata":{"id":"lvufrGi73p1U"},"source":["##Dropout layers\n","\n","\n","The term \"dropout\" is used for a technique which drops out some nodes of the network. Dropping out can be seen as temporarily deactivating or ignoring neurons of the network. This technique is applied in the training phase to reduce overfitting effects.\n","\n","Usually, dropout is placed on the fully connected layers only because they are the one with the greater number of parameters and thus they're likely to excessively co-adapting themselves causing overfitting. However, since it's a stochastic regularization technique, you can really place it everywhere.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"agek8m7G3pyo"},"source":["##How to handle over fitting in deep learning \n","\n","\n","* Simplifying The Model: The first step when dealing with overfitting is to decrease the complexity of the model. To decrease the complexity, we can simply remove layers or reduce the number of neurons to make the network smaller.\n","* Reduce the network's capacity by removing layers or reducing the number of elements in the hidden layers.\n","* Apply regularization , which comes down to adding a cost to the loss function for large weights.\n","* Early Stopping: Early stopping is a form of regularization while training a model with an iterative method, such as gradient descent. Since all the neural networks learn exclusively by using gradient descent, early stopping is a technique applicable to all the problems.\n","* Use Dropout layers, which will randomly remove certain features by setting them to zero.\n"]},{"cell_type":"markdown","metadata":{"id":"mWl9hRH53pv2"},"source":["##Can we implement the code from TF1 to TF2 environment ?if yes how\n","\n","\n","Implement TensorFlow 1.x code to TensorFlow 2.x.\n","\n","1. Replace v1.Session.run calls\n","\n","Every v1.Session.run call should be replaced by a Python function.\n","\n","* The feed_dict and v1.placeholders become function arguments.\n","* The fetches become the function's return value.\n","* During conversion eager execution allows easy debugging with standard Python tools like pdb.\n","\n","After that, add a tf.function decorator to make it run efficiently in graph. Check out the Autograph guide for more information about how this works.\n","\n","2. Use Python objects to track variables and losses\n","All name-based variable tracking is strongly discouraged in TensorFlow 2.x. Use Python objects to to track variables.\n","\n","Use tf.Variable instead of v1.get_variable.\n","\n","\n","3. Upgrade your training loops\n","Use the highest-level API that works for your use case. Prefer tf.keras.Model.fit over building your own training loops.\n","\n","4. Upgrade your data input pipelines\n","Use tf.data datasets for data input. These objects are efficient, expressive, and integrate well with tensorflow.\n","\n","5. Migrate off compat.v1 symbols\n","The tf.compat.v1 module contains the complete TensorFlow 1.x API, with its original semantics.\n","\n","The TensorFlow 2.x upgrade script will convert symbols to their v2 equivalents if such a conversion is safe, i.e., if it can determine that the behavior of the TensorFlow 2.x version is exactly equivalent (for instance, it will rename v1.arg_max to tf.argmax, since those are the same function).\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"f4j8_r9L8QbP"},"source":["##CNN\n","\n","\n","A Convolutional Neural Network (ConvNet/CNN) is a Deep Learning algorithm which can take in an input image, assign importance (learnable weights and biases) to various aspects/objects in the image and be able to differentiate one from the other.\n","\n","CNN is an efficient recognition algorithm which is widely used in pattern recognition and image processing.\n","\n","Convolutional neural networks are composed of multiple layers of artificial neurons. Artificial neurons, a rough imitation of their biological counterparts, are mathematical functions that calculate the weighted sum of multiple inputs and outputs an activation value. When you input an image in a ConvNet, each layer generates several activation functions that are passed on to the next layer.\n","\n","The first layer usually extracts basic features such as horizontal or diagonal edges. This output is passed on to the next layer which detects more complex features such as corners or combinational edges. As we move deeper into the network it can identify even more complex features such as objects, faces, etc.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"zyvgcyjD8QXm"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"vwewLRVN8QUe"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"W528Mvls8QRo"},"source":["##What is use of max pooling layers ?\n","\n","Max pooling is a sample-based discretization process. The objective is to down-sample an input representation.\n","\n","It is a pooling operation that selects the maximum element from the region of the feature map covered by the filter. Thus, the output after max-pooling layer would be a feature map containing the most prominent features of the previous feature map. "]},{"cell_type":"markdown","metadata":{"id":"vEjZvNpk5F3H"},"source":["##TF1 and TF2 difference\n","\n","**TensorFlow 1.x** The writing of code was divided into two parts: building the computational graph and later creating a session to execute it. this was quite cumbersome, especially if in the big model that you have designed, a small error existed somewhere in the beginning. \n","\n","**TensorFlow2.0** Eager Execution is implemented by default, i.e. you no longer need to create a session to run the computational graph,  you can see the result of your code directly without the need of creating Session.\n","\n","\n","Many old libraries (example tf.contrib) were removed, and some consolidated. For example, in TensorFlow1.x the model could be made using Contrib, layers, Keras or estimators, so many options for the same task confused many new users. TensorFlow 2.0 promotes TensorFlow Keras for model experimentation and Estimators for scaled serving, and the two APIs are very convenient to use."]},{"cell_type":"markdown","metadata":{"id":"dR3qphWu6aDK"},"source":["##Optimizers\n","\n","\n","\n","Optimizers are algorithms or methods used to minimize an error function(loss function)or to maximize the efficiency of production. Optimizers are mathematical functions which are dependent on model's learnable parameters i.e Weights & Biases.\n","\n","Few optimizers are:\n","* Batch gradient descent. Also known as vanilla gradient descent.\n","* Stochastic gradient descent.\n","* Mini batch gradient descent.\n","* Adagrad.\n","* Adadelta.\n","* RMSprop. \n","* Adam.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"30sNiuxj6aBU"},"source":["##How do u select datasets for training image classifications model\n","\n","A high-quality training dataset enhances the accuracy and speed of your decision-making while lowering the burden on your organization’s resources.\n","\n","You need to take into account 3 key aspects: the desired level of granularity within each label, the desired number of labels, and what parts of an image fall within the selected labels.\n","\n","Here are the questions to consider:\n","\n","1. What is your desired level of granularity within each label?\n","2. What is your desired number of labels for classification?\n","3. Which part of the images do you want to be recognized within the selected label?\n","\n","\n","In general, when it comes to machine learning, the richer your dataset, the better your model performs.\n","\n","In addition, the number of data points should be similar across classes in order to ensure the balancing of the dataset.\n","\n","However, how you define your labels will impact the minimum requirements in terms of dataset size"]},{"cell_type":"markdown","metadata":{"id":"O9MpQld96Z9G"},"source":["##What u do if datasets size is small\n","\n","\n","**Do some data augmentation.**\n","You can often extend your dataset by augmenting the data that you have. It’s about making slight changes to the data that should not significantly change the model output. For example, an image of a cat is still an image of a cat if it’s rotated 40 degrees.\n","\n","**Generate some synthetic data.**\n","If you have exhausted your options for augmenting real data, you could start thinking about creating some fake data. Generating synthetic data can also be a great way to cover some edge cases that your real dataset does not.\n","\n","**Beware of lucky splits.**\n","When training machine learning models, it is quite common to randomly split the dataset into train and test sets according to some ratio. Usually, this is fine. But when working with small datasets, there is a high risk of noise due to the low volume of training examples.\n","\n","In this scenario, k-fold Cross-Validation is a better choice.\n","\n","**Use transfer learning.**\n","If you’re working with a somewhat standardized data format like text, images, video or sound, you can leverage all the previous work others have put into these domains with transfer learning. It’s like standing on the shoulders of giants.\n","\n","**Try an ensemble of “weak learners”.**\n","Sometimes, you just have to face the fact that you do not have enough data to do anything fancy. Luckily, there are many traditional machine learning algorithms you can fall back to which are less sensitive to the size of your dataset."]},{"cell_type":"markdown","metadata":{"id":"9FtSaiBM6Z6i"},"source":["##Resnet, Inception and Efficient net?\n","\n","\n","**GoogLeNet/Inception**\n","\n","The GoogLeNet builds on the idea that most of the activations in a deep network are either unnecessary(value of zero) or redundant because of correlations between them. Therefore the most efficient architecture of a deep network will have a sparse connection between the activations, which implies that all 512 output channels will not have a connection with all the 512 input channels. There are techniques to prune out such connections which would result in a sparse weight/connection. But kernels for sparse matrix multiplication are not optimized in BLAS or CuBlas(CUDA for GPU) packages which render them to be even slower than their dense counterparts.\n","\n","So GoogLeNet devised a module called inception module that approximates a sparse CNN with a normal dense construction(shown in the figure). Since only a small number of neurons are effective as mentioned earlier, the width/number of the convolutional filters of a particular kernel size is kept small. Also, it uses convolutions of different sizes to capture details at varied scales(5X5, 3X3, 1X1).\n","\n","**Residual Networks**\n","\n","\n","As per what we have seen so far, increasing the depth should increase the accuracy of the network, as long as over-fitting is taken care of. But the problem with increased depth is that the signal required to change the weights, which arises from the end of the network by comparing ground-truth and prediction becomes very small at the earlier layers, because of increased depth. It essentially means that earlier layers are almost negligible learned. This is called vanishing gradient. The second problem with training the deeper networks is, performing the optimization on huge parameter space and therefore naively adding the layers leading to higher training error. Residual networks allow training of such deep networks by constructing the network through modules called residual models as shown in the figure. This is called degradation problem. The intuition around why it works can be seen as follows:\n","\n","\n","**EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks (2019)**\n","\n","EfficientNet is a convolutional neural network architecture and scaling method that uniformly scales all dimensions of depth/width/resolution using a compound coefficient. Unlike conventional practice that arbitrary scales these factors, the EfficientNet scaling method uniformly scales network width, depth, and resolution with a set of fixed scaling coefficients\n","\n","\n","|Model Name|Number of parameters|ImageNet Top 1 Accuracy|Year|\n","|-|-|-|-|\n","|Inception V1|5 M|69.8%|2014|\n","|Inception V2|11.2 M|74.8%|2015|\n","|ResNet-50|26 M|77.15%|2015|\n","|ResNet-152|60 M|78.57%|2015|\n","|Inception V3|27 M|78.8%|2015|\n","|NoisyStudent EfficientNet-L2|480 M|88.4%|2020|\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"uRM8vFFk6Z2n"},"source":["##YOLO difference ? v1 to v5\n","\n","**Yolo V1:**\n","\n","It uses Darknet framework which is trained on ImageNet-1000 dataset. This works as mentioned above but has many limitations because of it the use of the YOL v1 is restricted. It could not find small objects if they are appeared as a cluster. This architecture found difficulty in generalisation of objects if the image is of other dimensions different from the trained image. The major issue is localization of objects in the input image.\n","\n","**YOLO v2:**\n","\n","The second version of the YOLO is named as YOLO9000 which has been published by Joseph Redmon and Ali Farhadi at the end of 2016. The major improvements of this version are better , faster and more advanced to meet the Faster R-CNN which also an object detection algorithm which uses a Region Proposal Network to identify the objects from the image input and SSD(Single Shot Multibox Detector).\n","\n","The changes from YOLO to YOLO v2:\n","\n","Batch Normalization: it normalise the input layer by altering slightly and scaling the activations. Batch normalization decreases the shift in unit value in the hidden layer and by doing so it improves the stability of the neural network. By adding batch normalization to convolutional layers in the architecture MAP (mean average precision) has been improved by 2% .\n","\n","Higher Resolution Classifier: the input size in YOLO v2 has been increased from 224*224 to 448*448. The increase in the input size of the image has improved the MAP (mean average precision) upto 4%. This increase in input size is been applied while training the YOLO v2 architecture DarkNet 19 on ImageNet dataset.\n","\n","Anchor Boxes: one of the most notable changes which can visible in YOLO v2 is the introduction the anchor boxes. YOLO v2 does classification and prediction in a single framework.\n","\n","Fine-Grained Features: one of the main issued that has to be addressed in the YOLO v1 is that detection of smaller objects on the image. This has been resolved in the YOLO v2 divides the image into 13*13 grid cells which is smaller when compared to its previous version. This enables the yolo v2 to identify or localize the smaller objects in the image and also effective with the larger objects. \n","\n","Multi-Scale Training: on YOLO v1 has a weakness detecting objects with different input sizes which says that if YOLO is trained with small images of a particular object it has issues detecting the same object on image of bigger size. This has been resolved to a great extent in YOLO v2 where it is trained with random images with different dimensions range between 320*320 to 608*608. This allows the network to learn and predict the objects from various input dimensions with accuracy.\n","\n","\n","Darknet 19: YOLO v2 uses Darknet 19 architecture with 19 convolutional layers and 5 max pooling layers and a softmax layer for classification objects. \n","\n","\n","\n","\n","**YOLO v3:**\n","\n","The previous version has been improved for an incremental improvement which is now called YOLO v3. As many object detection algorithms are been there for a while now the competition is all about how accurate and quickly objects are detected. YOLO v3 has all we need for object detection in real-time with accurately and classifying the objects. The authors named this as an incremental improvement.\n","\n","Here we will have look what are the so called Incremental improvements in YOLO v3\n","\n","Bounding Box Predictions: In YOLO v3 gives the score for the objects for each bounding boxes. It uses logistic regression to predict the objectiveness score.\n","\n","Class Predictions: In YOLO v3 it uses logistic classifiers for every class instead of softmax which has been used in the previous YOLO v2. By doing so in YOLO v3 we can have multi-label classification.\n","\n","Feature Pyramid Networks (FPN): YOLO v3 makes predictions similar to the FPN where 3 predictions are made for every location the input image and features are extracted from each prediction. By doing so YOLO v3 has the better ability at different scales.\n","\n","Darknet-53: the predecessor YOLO v2 used Darknet-19 as feature extractor and YOLO v3 uses the Darknet-53 network for feature extractor which has 53 convolutional layers.\n","\n","\n","**YOLO v4**\n","\n","The 4th generation of YOLO has been released in April 2020.\n","\n","YOLO v4 takes the influence of state of art BoF (bag of freebies) and several BoS (bag of specials). The BoF improve the accuracy of the detector, without increasing the inference time. They only increase the training cost. On the other hand, the BoS increase the inference cost by a small amount however they significantly improve the accuracy of object detection.\n","\n","YOLO v4 also based on the Darknet and has obtained an AP value of 43.5 percent on the COCO dataset along with a real-time speed of 65 FPS on the Tesla V100, beating the fastest and most accurate detectors in terms of both speed and accuracy.\n","When compared with YOLO v3, the AP and FPS have increased by 10 percent and 12 percent, respectively.\n","\n","\n","**YOLO v5**\n","\n","After the release of YOLO v4, within just two months of period, an another version of YOLO has been released called YOLO v5 ! It is by the Glenn Jocher, who already known among the community for creating the popular PyTorch implementation of YOLO v3.\n","\n","YOLO v5 is different from all other prior releases, as this is a PyTorch implementation rather than a fork from original Darknet. Same as YOLO v4, the YOLO v5 has a CSP backbone and PA-NET neck. The major improvements includes mosaic data augmentation and auto learning bounding box anchors.\n","\n","Followings are some quotes by Joseph Nelson and Jacob Solawetz.\n","\n","`Running a Tesla P100, we saw inference times up to 0.007 seconds per image, meaning 140 frames per second (FPS)! By contrast, YOLO v4 achieved 50 FPS after having been converted to the same Ultralytics PyTorch library.`\n","\n","`YOLO v5 is small. Specifically, a weights file for YOLO v5 is 27 megabytes. Our weights file for YOLO v4 (with Darknet architecture) is 244 megabytes. YOLO v5 is nearly 90 percent smaller than YOLO v4.`\n","\n","\n","So, it said to be that YOLO v5 is extremely fast and lightweight than YOLO v4, while the accuracy is on par with the YOLO v4 benchmark.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ZL80oXp1gN3M"},"source":["##YOLO vs SSD\n","\n","**You only Look Once (YOLO)**\n","\n","It works solely on appearance at the image once to sight multiple objects. Thus, it’s referred to as YOLO, you merely Look Once. By simply gazing at the image once, the detection speed is in period (45 fps). Quick YOLOv1 achieves a hundred and fifty-five FPS. this is often another progressive deep learning object detection approach that has been printed in 2016 CVPR with quite 2000 citations. Yolo divides the image into a grid. For each grid, some values like class probabilities and the bounding box parameters are calculated.\n","\n","YOLO struggles to localize objects properly compared with quick R-CNN.YOLO has fewer background errors. quick R-CNN has thirteen.6% that the highest detections square measure false positive.\n","\n","**Single Shot Detector (SSD)**\n","\n","\n","By victimization SSD, we tend to solely have to be compelled to take one single shot to sight multiple objects inside the image, whereas regional proposal network (RPN) primarily based approaches like R-CNN series want 2 shots, one for generating region proposals, one for police work the article of every proposal. Thus, SSD is way quicker compared with two-shot RPN-based approaches. SSD not only uses one grid, but a combination of different sizes to better detect objects at any size.\n","\n","SSD, a single-shot detector for multiple classes that’s quicker than the previous progressive for single-shot detectors (YOLO), and considerably a lot of correct, really as correct as slower techniques that perform express region proposals and pooling (including quicker R-CNN).\n","\n","\n","|SSD|YOLO|\n","|-|-|\n","|Single Shot Detector|You Only Look Once|\n","|runs a convolutional network on input images at just one time and computes a feature map.|the open-source technique of object detection which will acknowledge objects in pictures and videos fleetly|\n","|SSD could be a higher choice as we have a tendency to square measure able to run it on a video and therefore the truth trade-off is extremely modest.|YOLO is a better option when  exactness is not too much of disquiet but you want to go super quick|\n","|When the object size is tiny, the performance dips a touch|YOLO could be a higher choice even when the object size is small.|\n","|runs a convolutional network on input image just one time and computes a feature map|can be enforced for applications as well as artificial intelligence, self-driving cars, and cancer recognition approaches.|\n"]},{"cell_type":"markdown","metadata":{"id":"-lg6fdNBgN1K"},"source":["##Mask RCNN \n","\n","\n","Mask RCNN is a deep neural network aimed to solve instance segmentation problem in machine learning or computer vision. In other words, it can separate different objects in a image or a video. You give it a image, it gives you the object bounding boxes, classes and masks.\n","\n","There are two stages of Mask RCNN. First, it generates proposals about the regions where there might be an object based on the input image. Second, it predicts the class of the object, refines the bounding box and generates a mask in pixel level of the object based on the first stage proposal. Both stages are connected to the backbone structure.\n","\n","In Mask RCNN we could actually force different layers in neural network to learn features with different scales, just like the anchors and ROIAlign, instead of treating layers as black box."]},{"cell_type":"markdown","metadata":{"id":"bDrjGVZBgNUS"},"source":["##Unet\n","\n","\n","\n","UNet is a convolutional neural network architecture that expanded with few changes in the CNN architecture. It was invented to deal with biomedical images where the target is not only to classify whether there is an infection or not but also to identify the area of infection.\n","\n","The architecture had two main parts that were encoder and decoder. The encoder is all about the covenant layers followed by pooling operation. It is used to extract the factors in the image. The second part decoder uses transposed convolution to permit localization. It is again an F.C connected layers network. "]},{"cell_type":"markdown","metadata":{"id":"LoeRlM07gNQ1"},"source":["##Advantages of SSD and YOLO\n","\n","\n","SSD can be trained end-to-end for better accuracy. SSD makes more predictions and has better coverage on location, scale, and aspect ratios. With the improvements above, SSD can lower the input image resolution to 300 × 300 with a comparative accuracy performance.\n","\n","\n","The biggest advantage of using YOLO is its superb speed – it's incredibly fast and can process 45 frames per second. YOLO also understands generalized object representation. This is one of the best algorithms for object detection and has shown a comparatively similar performance to the R-CNN algorithms."]},{"cell_type":"markdown","metadata":{"id":"-bQwSBoagNOH"},"source":["##How boundary box/anchor box is trained ?\n","\n","\n","Anchor boxes are a set of predefined bounding boxes of a certain height and width. These boxes are defined to capture the scale and aspect ratio of specific object classes you want to detect and are typically chosen based on object sizes in your training datasets. During detection, the predefined anchor boxes are tiled across the image. The network predicts the probability and other attributes, such as background, intersection over union (IoU) and offsets for every tiled anchor box. The predictions are used to refine each individual anchor box. You can define several anchor boxes, each for a different object size. Anchor boxes are fixed initial boundary box guesses.\n","\n","The network does not directly predict bounding boxes, but rather predicts the probabilities and refinements that correspond to the tiled anchor boxes. The network returns a unique set of predictions for every anchor box defined. The final feature map represents object detections for each class. The use of anchor boxes enables a network to detect multiple objects, objects of different scales, and overlapping objects."]},{"cell_type":"markdown","metadata":{"id":"-_HIggc9gNLY"},"source":["##Loss in boundary box\n","\n","In object detection task, we try to guide the computer to predict the objects and their location in a given image data. To accomplish this, we try to formulate a mechanism to mimic the behavior of “localizing” an object by enclosing them with a “rectangle” notation which usually called as a bounding box.\n","\n","These bounding boxes, usually annotated as 4 points of value represents either specific corner/center points of the coordinates of the bounding box or its width/height. The usual format of the annotation is either `(left, top, right, bottom), (left, top, width, height), or (center_x, center_y, width, height)`. Thus, the deep learning for object detection task is designed specifically to predict values related to those points by regressing them for the localization part.\n","\n","The coordinates of predicted and ground truth bounding boxes to measure the distance between bounding boxes.\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"cLqbwYPNgflO"},"source":["##How do u evaluate object detection model ? what is map ?\n","\n","\n","A object detection model produces the output in three components:\n","1. The bounding boxes — x1, y1, width, height if using the COCO file format\n","2. The class of the bounding box\n","3. The probability score for that prediction— how certain the model is that the class is actually the predicted class\n","\n","\n","To evaluate if an object was located we use the Intersection over Union (IoU) as a similarity measure. It’s given by the area of the overlap divided by the size of the union of the two bounding boxes.\n","\n","$$IOU= \\frac{Area of Overlap}{Area of Union}$$\n","\n","\n","You count the accumulated TP and the accumulated FP and compute the precision/recall at each line. Usually, the Average Precision is computed as the average precision at 11 equally spaced recall levels. The Mean Average Precision (mAP) is the averaged AP over all the object categories."]},{"cell_type":"markdown","metadata":{"id":"m9pj8cUp6Zzf"},"source":["##Select Random variables in pytorch - torch.rand\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DKtAkoO-6jlC","executionInfo":{"status":"ok","timestamp":1628194020449,"user_tz":-330,"elapsed":514,"user":{"displayName":"Jay Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghp15R1mmnoYYCixHxYvGt-QFWUeep9kyrxYR5p9A=s64","userId":"15713361043153944297"}},"outputId":"1e4f438b-4efa-4851-e97d-acbf6ba76ba3"},"source":["import torch\n","\n","print(torch.rand(6))\n","print(torch.rand(4,8))\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([0.3362, 0.7113, 0.9742, 0.2095, 0.8429, 0.8569])\n","tensor([[0.4578, 0.2394, 0.8099, 0.5807, 0.4100, 0.1511, 0.1304, 0.2033],\n","        [0.5307, 0.2237, 0.9872, 0.5843, 0.8137, 0.6815, 0.0775, 0.6166],\n","        [0.0689, 0.5292, 0.8240, 0.2161, 0.0614, 0.1220, 0.8183, 0.0215],\n","        [0.8301, 0.5542, 0.1201, 0.6280, 0.4653, 0.0279, 0.3458, 0.8449]])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Ybt9Q-b65F1g"},"source":["##Data augmentation\n","\n","\n","Data augmentation in data analysis are techniques used to increase the amount of data by adding slightly modified copies of already existing data or newly created synthetic data from existing data. It acts as a regularizer and helps reduce overfitting when training a machine learning model.\n"]},{"cell_type":"markdown","metadata":{"id":"pVxaPr_w6WXk"},"source":["##GAN\n","\n","\n","A generative adversarial network (GAN) is a machine learning (ML) model in which two neural networks compete with each other to become more accurate in their predictions. GANs typically run unsupervised and use a cooperative zero-sum game framework to learn.\n","\n","\n","**Working of GAN**\n","\n","The first step in establishing a GAN is to identify the desired end output and gather an initial training dataset based on those parameters. This data is then randomized and input into the generator until it acquires basic accuracy in producing outputs.\n","\n","After this, the generated images are fed into the discriminator along with actual data points from the original concept. The discriminator filters through the information and returns a probability between 0 and 1 to represent each image's authenticity (1 correlates with real and 0 correlates with fake). These values are then manually checked for success and repeated until the desired outcome is reached.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"KZedDHVJ6WVO"},"source":["##Some knn algorithm approach they asked to select a boundary box\n","\n"]},{"cell_type":"markdown","metadata":{"id":"kwsOII2q6WR9"},"source":["##Loss in the Image segmentation while masking - pixel loss\n","\n","\n","The most commonly used loss function for the task of image segmentation is a `pixel-wise cross entropy loss`. This loss examines each pixel individually, comparing the class predictions (depth-wise pixel vector) to our one-hot encoded target vector.\n","\n","Because the cross entropy loss evaluates the class predictions for each pixel vector individually and then averages over all pixels, we're essentially asserting equal learning to each pixel in the image. This can be a problem if your various classes have unbalanced representation in the image, as training can be dominated by the most prevalent class.\n","\n","Ronneberger et al (U-Net paper) discuss a loss weighting scheme for each pixel such that there is a higher weight at the border of segmented objects. This loss weighting scheme helped their U-Net model segment cells in biomedical images in a discontinuous fashion such that individual cells may be easily identified within the binary segmentation map.\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"bYaD_gD26WPL"},"source":["##Deep sort tracking\n","\n","A human tracks not just distance, velocity but also what that object looks like. Deep sort allows us to add this feature by computing deep features for every bounding box and using the similarity between deep features to also factor into the tracking logic.\n","\n","Deep SORT is an extension to SORT (Simple Real time Tracker). It has shown remarkable results in the Multiple Object Tracking (MOT) problem.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"c9yNhRmF5Fx1"},"source":["#NLP\n","\n","##Stemming and Lemmatization\n","\n","\n","**Stemming** just removes or stems the last few characters of a word, often leading to incorrect meanings and spelling. \n","\n","Ex: If you stem the word 'Caring', it would return 'Car'\n","\n","\n","**Lemmatization** considers the context and converts the word to its meaningful base form, which is called Lemma. \n","\n","Ex: If you lemmatize the word 'Caring', it would return 'Care'"]},{"cell_type":"markdown","metadata":{"id":"JSzefg6V5FwW"},"source":["##Word2vec, GLOve and Bert embedding difference\n","\n","\n","**Word2vec**\n","\n","Word2vec is used to group the vectors of similar words together in vectorspace. That is, it detects similarities mathematically. Word2vec creates vectors that are distributed numerical representations of word features, features such as the context of individual words.\n","\n","**GLOve**\n","\n","GloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space.\n","\n","**Bert embedding**\n","\n","It is new way to obtain pre-trained language model word representation. Many NLP tasks are benefit from BERT to get the SOTA. The goal of this project is to obtain the token embedding from BERT's pre-trained model.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"e7GFxSY15_6P"},"source":["##TFIDF\n","\n","TF-IDF is an abbreviation for Term Frequency Inverse Document Frequency. This is very common algorithm to transform text into a meaningful representation of numbers which is used to fit machine algorithm for prediction. \n","\n","$$TF-IDF= TF(t,d) * IDF(t)$$\n","\n","$$Where \\ tf(t,d) = \\sum_{x \\epsilon}fr(x,t)$$\n","$$Where fr(x,t)= \\left\\{\n","    \\begin{array}\\\\\n","        1 & \\mbox{if } \\ x=t \\\\\n","        0 & \\mbox{if } \\ otherwise\n","    \\end{array}\n","\\right.\n","$$\n","\n","Here the `tf(t,d)` returns is how many times is the term t present in document d.\n","\n","inverse document frequency(tf-idf) weight\n","\n","\n","$$idf(t)= log \\frac{|D|}{1+|{d:t \\epsilon d}|}$$\n","$$Where \\ |{d:t \\epsilon d}| \\ is \\ number \\ of \\ documents$$"]},{"cell_type":"markdown","metadata":{"id":"xdRU6pRr5_2h"},"source":["##RNN\n","\n","Recurrent neural networks, or RNNs, are a type of artificial neural network that add additional weights to the network to create cycles in the network graph in an effort to maintain an internal state.\n","\n","The promise of adding state to neural networks is that they will be able to explicitly learn and exploit context in sequence prediction problems, such as problems with an order or temporal component.\n","\n","Recurrent Neural Network(RNN) are a type of Neural Network where the output from previous step are fed as input to the current step.\n","\n","\n","Working of RNN:\n","\n","Suppose there is a deeper network with one input layer, three hidden layers and one output layer. Then like other neural networks, each hidden layer will have its own set of weights and biases, let’s say, for hidden layer 1 the weights and biases are (w1, b1), (w2, b2) for second hidden layer and (w3, b3) for third hidden layer. This means that each of these layers are independent of each other, i.e. they do not memorize the previous outputs.\n","\n","Now the RNN will do the following:\n","\n","* RNN converts the independent activations into dependent activations by providing the same weights and biases to all the layers, thus reducing the complexity of increasing parameters and memorizing each previous outputs by giving each output as input to the next hidden layer.\n","* Hence these three layers can be joined together such that the weights and bias of all the hidden layers is the same, into a single recurrent layer.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Sut4wNV85_0p"},"source":["##Difference between CNN and RNN\n","\n","\n","|CNN|RNN|\n","|-|-|\n","|It is suitable for spatial data such as images.|RNN is suitable for temporal data, also called sequential data.|\n","|This network takes fixed size inputs and generates fixed size outputs.|RNN can handle arbitrary input/output lengths.|\n","|CNN is a type of feed-forward artificial neural network with variations of multilayer perceptrons designed to use minimal amounts of preprocessing.|RNN unlike feed forward neural networks - can use their internal memory to process arbitrary sequences of inputs.|\n","|CNNs use connectivity pattern between the neurons. This is inspired by the organization of the animal visual cortex.|Recurrent neural networks use time-series information - what a user spoke last will impact what he/she will speak next.|\n","|CNNs are ideal for images and video processing.|RNNs are ideal for text and speech analysis.|\n"]},{"cell_type":"markdown","metadata":{"id":"yKksfqs75_xD"},"source":["##LSTM\n","\n","Long Short-Term Memory (LSTM) networks are a type of recurrent neural network capable of learning order dependence in sequence prediction problems. This is a behavior required in complex problem domains like machine translation, speech recognition, and more.\n","\n","The Long Short Term Memory architecture was motivated by an analysis of error flow in existing RNNs which found that long time lags were inaccessible to existing architectures, because backpropagated error either blows up or decays exponentially.\n","\n","An LSTM layer consists of a set of recurrently connected blocks, known as memory blocks. These blocks can be thought of as a differentiable version of the memory chips in a digital computer. Each one contains one or more recurrently connected memory cells and three multiplicative units – the input, output and forget gates – that provide continuous analogues of write, read and reset operations for the cells. … The net can only interact with the cells via the gates.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"DbH2nP_S5_vC"},"source":["##GRU\n","\n","\n","\n","GRU (Gated Recurrent Unit) is an improved version of standard recurrent neural network. It aims to solve the vanishing gradient problem which comes with a standard recurrent neural network. GRU can also be considered as a variation on the LSTM because both are designed similarly and, in some cases, produce equally excellent results.\n","\n","To solve the vanishing gradient problem of a standard RNN, GRU uses, so-called, update gate and reset gate. Basically, these are two vectors which decide what information should be passed to the output. The special thing about them is that they can be trained to keep information from long ago, without washing it through time or remove information which is irrelevant to the prediction.\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"HnAchh_R5_rj"},"source":["##Time series approach using encoder layers\n","\n","\n","The input to the encoder network is of the shape (sequence length, n_values), therefore each item in the sequence is made of n values. In constructing these values, different types of features are treated differently.\n","\n","`Time dependant features` — These are the features that vary with time, such as sales, and DateTime features. In the encoder, each sequential time dependant value is fed into an RNN cell.\n","\n","The input sequence with this feature can be fed into the recurrent network."]},{"cell_type":"markdown","metadata":{"id":"-0uRcRcS5_pD"},"source":["##Attentions \n","\n","\n","\n","Attention Mechanism is an attempt to implement the action of selectively concentrating on a few relevant things, while ignoring others in deep neural networks. \n","\n","In this Bidirectional LSTMs are used to generates a sequence of annotations (h1, h2,….., hTx) for each input sentence. All the vectors h1,h2.., etc., used in their work are basically the concatenation of forward and backward hidden states in the encoder.\n","\n","Here all the vectors h1,h2,h3…., hTx are representations of Tx number of words in the input sentence. In the simple encoder and decoder model, only the last state of the encoder LSTM was used (hTx in this case) as the context vector.\n","\n","The weights are also learned by a feed-forward neural network and I’ve mentioned their mathematical equation below.\n","\n","The context vector ci for the output word yi is generated using the weighted sum of the annotations:\n","\n","$$c_i =\\sum_{j=1}^{T_x}\\alpha_{ij}h_j$$"]},{"cell_type":"markdown","metadata":{"id":"uKF62oSN5_m3"},"source":["##Transformer architecture\n","\n","The paper ‘Attention Is All You Need’ introduces a novel architecture called Transformer. It uses the attention-mechanism we saw earlier. Like LSTM, Transformer is an architecture for transforming one sequence into another one with the help of two parts (Encoder and Decoder), but it differs from the previously described/existing sequence-to-sequence models because it does not imply any Recurrent Networks (GRU, LSTM, etc.).\n","\n","Transformer adopts an encoder-decoder architecture. The encoder consists of encoding layers that process the input iteratively one layer after another, while the decoder consists of decoding layers that do the same thing to the encoder's output.\n","\n","The function of each encoder layer is to generate encodings that contain information about which parts of the inputs are relevant to each other. It passes its encodings to the next encoder layer as inputs. Each decoder layer does the opposite, taking all the encodings and using their incorporated contextual information to generate an output sequence. To achieve this, each encoder and decoder layer makes use of an attention mechanism.\n","\n","For each input, attention weighs the relevance of every other input and draws from them to produce the output. Each decoder layer has an additional attention mechanism that draws information from the outputs of previous decoders, before the decoder layer draws information from the encodings.\n","\n","Both the encoder and decoder layers have a feed-forward neural network for additional processing of the outputs, and contain residual connections and layer normalization steps."]},{"cell_type":"markdown","metadata":{"id":"12BJZILr5_jE"},"source":["##Single headed and muti headed\n","\n","\n","**Single Headed Attention RNN: Stop Thinking With Your Head:**\n","\n","BERT, the current SotA architecture for NLP problems uses transformers and multi-headed attention modules to prevent the usage of RNN, which makes it more more stable during training and removes temporal dependencies between words. It does, also, add a lot of parameters to the model because the RNN is replaced with a (much bigger) feed-forward (FF) network.\n","\n","The author decided instead keep using RNNs, namely a single layer LSTM module with a single head of attention and a “Boom” module, apparently similar to a Transformer module.\n","\n","\n","**Multi-Head Self-Attention:**\n","\n","The aim here is to combine the knowledge explored by multiple heads or agents instead of doing it by one, as in the traditional case.\n","\n","Mathematically, it relates to attending to not only the different words of the sentence, but to different segments of the words, too. The words vectors are divided into a fixed number (h, number of heads) of chunks, and then self-attention is applied on the corresponding chunks, resulting in h context vector for each word. The final context vector is obtained by concatenating all those h context vectors."]},{"cell_type":"markdown","metadata":{"id":"nF9WQLCl5_gQ"},"source":["##BERT vs GPT\n","\n","\n","|BERT|GPT|\n","|-|-|\n","|Bidirectional in nature.|Autoregressive in nature.|\n","|With BERT, users can train their own question answering models in about 30 minutes on a single Cloud TPU, and in a few hours, using a single GPU.|GPT-3 showcases how a language model trained on a massive range of data can solve various NLP tasks without fine-tuning.|\n","|Comes with significant applications like Google Docs, Gmail Smart Compose etc.|Can be applied to write news, generate articles as well as codes.|\n","|Achieved a General Language Understanding Evaluation (GLUE) score of 80.4% and a 93.3% accuracy on SQuAD dataset.|Achieved a score of 81.5 F1 on conversational question answering benchmark in zero-shot learning; 84.0 F1 in one-shot learning; and 85.0 F1 in few-shot learning.|\n","||Achieved 64.3% accuracy on TriviaAQ benchmark and 76.2% accuracy on LAMBADA, with zero-shot learning.|\n","\n"]},{"cell_type":"markdown","metadata":{"id":"4ZGRtZMI3_J1"},"source":["##BART, t5, pegasus, XL net \n","\n","\n","**BART**\n","\n","BART is a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by \n","1. corrupting text with an arbitrary noising function.\n","2. learning a model to reconstruct the original text.\n","\n","BART is pre-trained by minimizing the cross-entropy loss between the decoder output and the original sequence.\n","\n","BART accepts any type of noising / text corruption for training the model.\n","* It allows Token Masking, where certain tokens are replaced with <Mask>.\n","* It allows Token Deletion, where certain tokens are deleted. Here, the model must learn which inputs are missing, which is a more difficult task.\n","* Text Infilling is also allowed; here, multiple tokens are replaced with one <Mask> token\n","* Sentence permutation, where sentences are determined to be sentences based on a full stop token (.). The sentences are then shuffled randomly.\n","* Document rotation, where a random token is selected and where the document is then built around this token.\n","\n","\n","**T5**\n","\n","T5 which stands for text to text transfer transformer makes it easy to fine tune a transformer model on any text to text task. Any NLP task event if it is a classification task, can be framed as an input text to output text problem.\n","\n","T5 is an encoder-decoder model and converts all NLP problems into a text-to-text format. It is trained using teacher forcing. This means that for training we always need an input sequence and a target sequence. The input sequence is fed to the model using input_ids .\n","\n","T5 also trains with the same objective as that of BERT's which is the Masked Language Model with a little modification to it. Masked Language Models are Bidirectional models, at any time t the representation of the word is derived from both left and the right context of it.2\n","\n","**pegasus**\n","\n","It is a State-of-the-Art Model for Abstractive Text Summarization. This abstractive text summarization is one of the most challenging tasks in natural language processing, involving understanding of long passages, information compression, and language generation.\n","\n","**XL Net**\n","\n","XLNet is an auto-regressive language model which outputs the joint probability of a sequence of tokens based on the transformer architecture with recurrence.\n","\n","XLNet is a large bidirectional transformer that uses improved training methodology, larger data and more computational power to achieve better than BERT prediction metrics on 20 language tasks. This is in contrast to BERT's masked language model where only the masked (15%) tokens are predicted.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"sggl7eZG3-1x"},"source":["##Images like ( person standing beside the car ) and textual information given - we have to map .. Approaches for this Encoder we use images and decoder side we use textual information he said"]},{"cell_type":"markdown","metadata":{"id":"41If71zp3-yF"},"source":["##Finally project explanation is very important "]},{"cell_type":"markdown","metadata":{"id":"0xVyH3_93-vD"},"source":[""]},{"cell_type":"code","metadata":{"id":"WYYlqqDV7wWK","executionInfo":{"status":"ok","timestamp":1628242261834,"user_tz":-330,"elapsed":2,"user":{"displayName":"Jay Kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghp15R1mmnoYYCixHxYvGt-QFWUeep9kyrxYR5p9A=s64","userId":"15713361043153944297"}}},"source":[""],"execution_count":null,"outputs":[]}]}